{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Only State Information: Calculating Nash equilibria '''\n",
    "\n",
    "epsilon = 0\n",
    "# determinstic_strategy_itertools_no_info = itertools.product([0,1], repeat = 1)\n",
    "determinstic_strategy_itertools_no_info = itertools.product([1,0], repeat = 1)\n",
    "\n",
    "determinisic_strategy_lists_no_info = list(list(strat) for strat in determinstic_strategy_itertools_no_info)\n",
    "all_determinstic_strategy_dictionary_no_info = {str(strat):strat for strat in determinisic_strategy_lists_no_info}\n",
    "strategy_set_p1_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "strategy_set_p2_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "\n",
    "strategy_set_p1_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p1_no_info.items()}\n",
    "strategy_set_p2_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p2_no_info.items()}\n",
    "\n",
    "print(strategy_set_p1_both_only_state)\n",
    "\n",
    "mode = 'ecological'\n",
    "ecopg = BaseEcologicalPublicGood(m = -6)\n",
    "\n",
    "information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_state(strategy_set_p1_both_only_state, strategy_set_p2_both_only_state,mode)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "print(payoffs_row)\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "print(payoffs_col)\n",
    "game_only_state = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result_only_state = pygambit.nash.enumpure_solve(game_only_state)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "print(len(result_only_state.equilibria))\n",
    " \n",
    "for i in result_only_state.equilibria:\n",
    "    print(i)  \n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "'''Only State Information: Calculating Nash equilibria '''\n",
    "E = 0.0001\n",
    "determinstic_strategy_itertools_no_info = itertools.product([1 - E,0 + E], repeat = 4)\n",
    "\n",
    "determinisic_strategy_lists_no_info = list(list(strat) for strat in determinstic_strategy_itertools_no_info)\n",
    "all_determinstic_strategy_dictionary_no_info = {str(strat):strat for strat in determinisic_strategy_lists_no_info}\n",
    "strategy_set_p1_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "strategy_set_p2_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "\n",
    "strategy_set_p1_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p1_no_info.items()}\n",
    "strategy_set_p2_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p2_no_info.items()}\n",
    "\n",
    "\n",
    "print(strategy_set_p1_no_info)\n",
    "\n",
    "# mode = 'only_state_information'\n",
    "# ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "\n",
    "# information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "# mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.05, discount_factors= 0)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_state_test(strategy_set_p1_no_info, strategy_set_p2_no_info, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "# payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "print(payoffs_row, \"payoff row\")\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "print(payoffs_col, \"payoff col\")\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "# print(len(result.equilibria))\n",
    " \n",
    "# for i in result.equilibria:\n",
    "#     print(i)  \n",
    "#     print(\"---\")\n",
    "\n",
    "process_and_print_nash_equilibria_results(result.equilibria, strategy_set_p1_no_info, strategy_set_p2_no_info)\n",
    "\n",
    "from pyCRLD.Environments.HistoryEmbedding import HistoryEmbedded\n",
    "memo1pd = HistoryEmbedded(socdi, h=(1,1,1))\n",
    "\n",
    "\n",
    "'''Only State Information: Calculating Nash equilibria '''\n",
    "E = 0.0001\n",
    "determinstic_strategy_itertools_no_info = itertools.product([1 - E,0 + E], repeat = 4)\n",
    "\n",
    "determinisic_strategy_lists_no_info = list(list(strat) for strat in determinstic_strategy_itertools_no_info)\n",
    "all_determinstic_strategy_dictionary_no_info = {str(strat):strat for strat in determinisic_strategy_lists_no_info}\n",
    "strategy_set_p1_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "strategy_set_p2_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "\n",
    "strategy_set_p1_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p1_no_info.items()}\n",
    "strategy_set_p2_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p2_no_info.items()}\n",
    "\n",
    "\n",
    "print(strategy_set_p1_no_info)\n",
    "\n",
    "# mode = 'only_state_information'\n",
    "# ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "\n",
    "# information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "# mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.05, discount_factors= 0)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_state_test(strategy_set_p1_no_info, strategy_set_p2_no_info, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "# payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "print(payoffs_row, \"payoff row\")\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "print(payoffs_col, \"payoff col\")\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "# print(len(result.equilibria))\n",
    " \n",
    "# for i in result.equilibria:\n",
    "#     print(i)  \n",
    "#     print(\"---\")\n",
    "\n",
    "process_and_print_nash_equilibria_results(result.equilibria, strategy_set_p1_no_info, strategy_set_p2_no_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitons of standard strategies\n",
    "\n",
    "\n",
    "WSLS = [1,0,0,1]\n",
    "GT = [1,0,0,0]\n",
    "ALLC = [1,1,1,1]\n",
    "TFT = [1,0,1,0]\n",
    "\n",
    "ALLD = [0,0,0,0]\n",
    "ReverseGT = [0,0,0,1]\n",
    "\n",
    "# strategy_set_p1_only_action =  {\n",
    "#     'WSLS': WSLS,\n",
    "#     'GT': GT,\n",
    "#     'ALLC': ALLC,\n",
    "#     'ALLD': ALLD,\n",
    "#     'ReverseGT': ReverseGT\n",
    "# }\n",
    "# strategy_set_p2_only_action =  {\n",
    "#     'WSLS': WSLS,\n",
    "#     'GT': GT,\n",
    "#     'ALLC': ALLC,\n",
    "#     'ALLD': ALLD,\n",
    "#     'ReverseGT': ReverseGT\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicator = replicator_dynamics(p1_reward_matrix_only_action, p2_reward_matrix_only_action, strategy_set_p1_only_action, strategy_set_p2_only_action)\n",
    "# with np.printoptions(precision=10, suppress=True):\n",
    "#         print(replicator[999])\n",
    "        \n",
    "# for k in strategy_set_p1_only_action:\n",
    "#     if np.round(replicator[999][k]) != 0:\n",
    "#         print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nashpy equlibria calculation\n",
    "\n",
    "\n",
    "\n",
    "def calculate_nash_equilibria(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    game = nashpy.Game(p1_reward_matrix, p2_reward_matrix)\n",
    "\n",
    "    p1_list_of_strategies = list(strategy_set_p1.keys())\n",
    "    p2_list_of_strategies = list(strategy_set_p2.keys())\n",
    "\n",
    "    # equilibria = game.support_enumeration()\n",
    "    equilibria = game.vertex_enumeration()\n",
    "\n",
    "\n",
    "    # pure_equilibria = []\n",
    "    # for eq in equilibria:\n",
    "    #     if all((prob == 1.0 or prob == 0.0) for prob in eq[0]) and all((prob == 1.0 or prob == 0.0) for prob in eq[1]):\n",
    "    #         pure_equilibria.append(eq)\n",
    "\n",
    "    print(\" Nash Equilibria\")\n",
    "    list_of_eq_strategies = []\n",
    "    for eq in equilibria:\n",
    "        # print(\"Player 1 strategy:\", eq[0].astype(bool), \"Player 2 strategy:\", eq[1])\n",
    "\n",
    "        p1_active_strategy = np.array(p1_list_of_strategies)[eq[0].astype(bool)]\n",
    "        p2_active_strategy = np.array(p2_list_of_strategies)[eq[1].astype(bool)]\n",
    "\n",
    "        # print(\"P1:\", p1_active_strategy, \"P2:\", p2_active_strategy)\n",
    "        list_of_eq_strategies.append({\"P1\" : p1_active_strategy, \"P2:\":p2_active_strategy})\n",
    "\n",
    "    return equilibria, list_of_eq_strategies\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replicator_dynamics(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    game = nashpy.Game(p1_reward_matrix, p2_reward_matrix)\n",
    "\n",
    "    p1_list_of_strategies = list(strategy_set_p1.keys())\n",
    "    p2_list_of_strategies = list(strategy_set_p2.keys())\n",
    "\n",
    "    # mutation_rate = np.full((16,16),0.005)\n",
    "    dynamics = game.replicator_dynamics(y0=np.random.rand(16))\n",
    "    print(dynamics[0::100])\n",
    "\n",
    "    return dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_four_conditions_higher_f(num_samples=5, degraded_choice = False, m_value = -6, discount_factor = 0.98, exclude_degraded_state_for_average_cooperation = True):\n",
    "    \"\"\"\n",
    "    Runs simulations for different information conditions and outputs \n",
    "    the results for each condition.\n",
    "    \n",
    "    Parameters:\n",
    "        ecopg (EcologicalPublicGood): An instance of the ecological public good model.\n",
    "        num_samples (int): Number of initial conditions to sample.\n",
    "        Tmax (int): Maximum time steps for trajectory simulation.\n",
    "        tolerance (float): Convergence tolerance for fixed point detection.\n",
    "        \n",
    "    Returns:\n",
    "        None (prints the output summaries for each information condition)\n",
    "    \"\"\"\n",
    "\n",
    "    print(locals())\n",
    "    \n",
    "    information_modes = [\n",
    "        'both_state_and_action_information', \n",
    "        'only_action_history_information', \n",
    "        'only_state_information', \n",
    "        'no_information'\n",
    "    ]\n",
    "\n",
    "    basin_of_attraction_and_avg_cooperation_results = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    ecopg = EcologicalPublicGood(N=2,\n",
    "                                 f=2, \n",
    "                                 c=5, \n",
    "                                 m= m_value,\n",
    "                                 qc=0.02, \n",
    "                                 qr= 0.0001, \n",
    "                                 degraded_choice = degraded_choice)\n",
    "\n",
    "\n",
    "    for mode in information_modes:\n",
    "        # Initialize the information condition\n",
    "        information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "        mae = POstratAC_eps(env=information_condition_instance, learning_rates=0.1, discount_factors= discount_factor)\n",
    "\n",
    "        # Data storage\n",
    "\n",
    "        # print(f\"\\nMode: {mode}\")\n",
    "\n",
    "        avg_coop_time_pairs = run_simulation_across_conditions(\n",
    "            mae = mae, \n",
    "            mode = mode,\n",
    "            num_samples = num_samples, \n",
    "            exclude_degraded_state_for_average_cooperation = exclude_degraded_state_for_average_cooperation\n",
    "        )\n",
    "\n",
    "        # Create DataFrame for processing\n",
    "        df = pd.DataFrame(avg_coop_time_pairs, columns=[\"AverageCooperation\", \"TimeToReach\"])\n",
    "        total_count = len(df)\n",
    "        # print(df)\n",
    "\n",
    "\n",
    "        average_cooperation_across_initial_conditions = df['AverageCooperation'].agg('mean')\n",
    "        # print(\"Mean Final Cooperation Across Initial Conditions:\", np.round(average_cooperation_across_initial_conditions,2))\n",
    "\n",
    "        # Classification function\n",
    "    \n",
    "\n",
    "        df['Classification'] = df['AverageCooperation'].apply(lambda x: \"Defection\" if x < 0.4 else \"Cooperation\" if x > 0.6 else \"Mixed\" )\n",
    "\n",
    "        # Summary statistics\n",
    "        basin_of_attraction_size = df.groupby('Classification')['TimeToReach'].agg(\n",
    "            MedianTimetoReach='median',\n",
    "            Percentage=lambda x: round((len(x) / total_count) * 100, 1)\n",
    "        ).reset_index()\n",
    "\n",
    "        basin_of_attraction_and_avg_cooperation_results[mode] = {\n",
    "        \"average_cooperation\": np.round(average_cooperation_across_initial_conditions, 3),\n",
    "        \"basin_of_attraction_size\": basin_of_attraction_size\n",
    "        }\n",
    "\n",
    "\n",
    "    return basin_of_attraction_and_avg_cooperation_results\n",
    "\n",
    "data = compare_four_conditions_higher_f()\n",
    "print(get_cooperation_basin_size(data['only_action_history_information']))\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_degraded_states_from_obsdist_and_normalise(obsdist, Oset):\n",
    "   \n",
    "        # Exclude degraded states from the observation distribution\n",
    "\n",
    "    degraded_mask = jnp.array(['g' in label for label in Oset])\n",
    "    obsdist = jnp.where(degraded_mask, 0, obsdist)\n",
    "\n",
    "    # Normalize rows to ensure sum of probabilities is 1\n",
    "    row_sums = jnp.sum(obsdist, axis=1, keepdims=True)\n",
    "    obsdist_without_degraded_state = jnp.where(row_sums > 0, obsdist / row_sums, obsdist)  # Avoid division by zero\n",
    "\n",
    "    return obsdist_without_degraded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#only to check if i can replicate the results\n",
    "\n",
    "ecopg = EcologicalPublicGood(N=2, f=1.2, c=5, m=-4, qc=0.2, qr= 0.1, degraded_choice=True)\n",
    "\n",
    "ecopg_with_history = HistoryEmbedded(ecopg, h = (1,1,1))\n",
    "\n",
    "for mode in ['both_state_and_action_information', 'only_action_history_information', 'only_state_information', 'no_information']:\n",
    " \n",
    "    # Initialize the information condition\n",
    "    information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "    mae = POstratAC(env=information_condition_instance, learning_rates=0.1, discount_factors=0.9)\n",
    "\n",
    "    # Data storage\n",
    "    \n",
    "    avg_coop_time_pairs = []\n",
    "    num_samples = 10\n",
    "    initial_conditions_list = lhs_sampling(mae.Q, num_samples, mae.N)\n",
    "\n",
    "    print(f\"\\nMode: {mode}\")\n",
    "\n",
    "    # Monte Carlo Simulations\n",
    "    for initial_condition in initial_conditions_list:\n",
    "\n",
    "        # initial_condition = make_degraded_state_cooperation_probablity_one(initial_condition, information_condition_instance.Oset[0]) #to make sure all of them start at the same position in the degraded state (shouldn't it)\n",
    "        xtraj, fixedpointreached = mae.trajectory(initial_condition, Tmax=10000, tolerance=1e-5)\n",
    "        final_point = xtraj[-1]\n",
    "        \n",
    "        avg_coop_across_states = get_average_cooperativeness_including_degraded_state(policy=final_point, obsdist=mae.obsdist(final_point), mode = mode, Oset = mae.env.Oset[0])[0]\n",
    "        time_to_reach = xtraj.shape[0]\n",
    "\n",
    "        # Store cooperativeness and time as pairs (round cooperativeness to 2 decimals)\n",
    "        avg_coop_time_pairs.append((round(avg_coop_across_states, 2), time_to_reach))\n",
    "\n",
    "    # Create DataFrame for processing\n",
    "    df = pd.DataFrame(avg_coop_time_pairs, columns=[\"AverageCooperation\", \"TimeToReach\"])\n",
    "    total_count = len(df)\n",
    "    \n",
    "    average_cooperation_across_initial_conditions = np.round(df['AverageCooperation'].agg('mean'), 3)\n",
    "    print(\"Mean Final Cooperation Across Initial Conditions \", average_cooperation_across_initial_conditions)\n",
    "\n",
    "\n",
    "    # Add a classification column\n",
    "    def classify(avg_coop):\n",
    "        if avg_coop < 0.1:\n",
    "            return \"Defection\"\n",
    "        elif avg_coop > 0.9:\n",
    "            return \"Cooperation\"\n",
    "        else:\n",
    "            return \"Mixed\"\n",
    "\n",
    "\n",
    "    df['Classification'] = df['AverageCooperation'].apply(classify)\n",
    "    average_cooperation_across_initial_conditions = df['AverageCooperation'].agg('mean')\n",
    "    # Reporting unique entries\n",
    "\n",
    "    # Overall Summary\n",
    "    summary = df.groupby('Classification')['TimeToReach'].agg(\n",
    "        MedianTimetoReach='median',\n",
    "        Percentage= lambda x: round((len(x) / total_count) * 100,1)\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(summary)\n",
    "\n",
    "'''\n",
    "Output: \n",
    "\n",
    "Mode: both_state_and_action_information\n",
    "Mean Final Cooperation Across Initial Conditions  0.88\n",
    "  Classification  MedianTimetoReach  Percentage\n",
    "0    Cooperation              102.0        63.6\n",
    "1          Mixed              295.5        36.4\n",
    "\n",
    "Mode: only_action_history_information\n",
    "Mean Final Cooperation Across Initial Conditions  0.156\n",
    "  Classification  MedianTimetoReach  Percentage\n",
    "0    Cooperation               59.0         3.6\n",
    "1      Defection             1842.0        60.0\n",
    "2          Mixed             2856.0        36.4\n",
    "\n",
    "Mode: only_state_information\n",
    "Mean Final Cooperation Across Initial Conditions  0.85\n",
    "  Classification  MedianTimetoReach  Percentage\n",
    "0    Cooperation              101.5        54.5\n",
    "1          Mixed              223.0        45.5\n",
    "\n",
    "Mode: no_information\n",
    "Mean Final Cooperation Across Initial Conditions  0.005\n",
    "  Classification  MedianTimetoReach  Percentage\n",
    "0      Defection             1832.0       100.0\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(mae, information_condition_instance, num_samples, initial_cooperation_in_degraded_state, include_degraded_state_for_average_cooperation):\n",
    "    \"\"\"\n",
    "    Runs Monte Carlo simulations and collects average cooperation and time-to-reach data.\n",
    "\n",
    "    Parameters:\n",
    "        mae: The POstratAC instance (learning agent).\n",
    "        information_condition_instance: The instance of Information_Conditions.\n",
    "        num_samples (int): Number of initial conditions to sample.\n",
    "        initial_cooperation_in_degraded_state (int): If 0, cooperation in degraded state is set to zero; \n",
    "                                                     if 1, it is set to one; otherwise, no changes.\n",
    "        include_degraded_state_for_average_cooperation (bool): Whether to include the degraded state in average cooperation.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (average cooperation, time-to-reach) tuples.\n",
    "    \"\"\"\n",
    "    avg_coop_time_pairs = []\n",
    "    initial_conditions_list = lhs_sampling(mae.Q, num_samples, mae.N)\n",
    "\n",
    "    for initial_condition in initial_conditions_list:\n",
    "        if initial_cooperation_in_degraded_state == 0:\n",
    "            initial_condition = make_degraded_state_cooperation_probablity_zero(\n",
    "                initial_condition, information_condition_instance.Oset[0]\n",
    "            )\n",
    "        elif initial_cooperation_in_degraded_state == 1:\n",
    "            initial_condition = make_degraded_state_cooperation_probablity_one(\n",
    "                initial_condition, information_condition_instance.Oset[0]\n",
    "            )\n",
    "\n",
    "        xtraj, fixedpointreached = mae.trajectory(initial_condition, Tmax=10000, tolerance=1e-5)\n",
    "        final_point = xtraj[-1]\n",
    "\n",
    "        if include_degraded_state_for_average_cooperation:  \n",
    "                avg_coop_across_states = get_average_cooperativeness_excluding_degraded_state(\n",
    "                    policy=final_point, \n",
    "                    obsdist=mae.obsdist(final_point), \n",
    "                    mode=mode, \n",
    "                    Oset=mae.env.Oset[0]\n",
    "                )[0]\n",
    "\n",
    "        else:\n",
    "                avg_coop_across_states = get_average_cooperativeness_excluding_degraded_state(\n",
    "                    policy=final_point, \n",
    "                    obsdist=mae.obsdist(final_point), \n",
    "                    mode=mode, \n",
    "                    Oset=mae.env.Oset[0]\n",
    "                )[0]\n",
    "\n",
    "\n",
    "        time_to_reach = xtraj.shape[0]\n",
    "\n",
    "        avg_coop_time_pairs.append((round(avg_coop_across_states, 2), time_to_reach))\n",
    "\n",
    "    return avg_coop_time_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"adjust initial condition degraded state\"\n",
    "\n",
    "def adjust_initial_condition_degraded_state(initial_condition, Oset, adjustment):\n",
    "\n",
    "    if adjustment == \"make_degraded_state_cooperation_probablity_zero\":\n",
    "\n",
    "        degraded_mask = jnp.array(['g' in label for label in Oset])\n",
    "        initial_condition[:, degraded_mask, 0] = 0\n",
    "        initial_condition[:, degraded_mask, 1] = 1\n",
    "\n",
    "    if adjustment == \"make_degraded_state_cooperation_probablity_one\"\n",
    "\n",
    "    return initial_condition\n",
    "\n",
    "def make_degraded_state_cooperation_probablity_one(initial_condition, Oset):\n",
    "\n",
    "    degraded_mask = jnp.array(['g' in label for label in Oset])\n",
    "    initial_condition[:, degraded_mask, 0] = 1\n",
    "    initial_condition[:, degraded_mask, 1] = 0\n",
    "\n",
    "    return initial_condition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Store results for different m values\n",
    "results = []\n",
    "\n",
    "for m_value in range(0, -7, -2):  # Loop from 0 to -6\n",
    "    print(f\"\\nRunning for m = {m_value}\")\n",
    "\n",
    "    # Initialize the ecological model for the current m\n",
    "    ecopg = EcologicalPublicGood(N=2, f=1.2, c=5, m= m_value, qc=0.02, qr= 0.0001, degraded_choice=False)\n",
    "\n",
    "    # Store cooperation basin sizes for different informational conditions\n",
    "    cooperation_basin_sizes = {}\n",
    "\n",
    "    for mode in ['both_state_and_action_information', 'only_action_history_information', 'only_state_information', 'no_information']:\n",
    "        \n",
    "        information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "        mae = POstratAC(env=information_condition_instance, learning_rates=0.1, discount_factors=0.99)\n",
    "\n",
    "        avg_coop_values = []\n",
    "        num_samples = 5\n",
    "        initial_conditions_list = lhs_sampling(mae.Q, num_samples, mae.N)\n",
    "\n",
    "        for initial_condition in initial_conditions_list:\n",
    "            initial_condition = make_degraded_state_cooperation_probablity_zero(initial_condition, information_condition_instance.Oset[0]) #to make sure all of them start at the same position in the degraded state (shouldn't it)\n",
    "            xtraj, _ = mae.trajectory(initial_condition, Tmax=10000, tolerance=1e-5)\n",
    "            final_point = xtraj[-1]\n",
    "            avg_coop = get_average_cooperativeness_excluding_degraded_state(policy=final_point, obsdist=mae.obsdist(final_point), mode=mode, Oset=mae.env.Oset[0])[0]\n",
    "            avg_coop_values.append(round(avg_coop, 3))\n",
    "\n",
    "        df = pd.DataFrame(avg_coop_values, columns=[\"AverageCooperation\"])\n",
    "        \n",
    "        # Classification\n",
    "        df['Classification'] = np.where(df['AverageCooperation'] > 0.9, \"Cooperation\", \n",
    "                                        np.where(df['AverageCooperation'] < 0.1, \"Defection\", \"Mixed\"))\n",
    "    \n",
    "        # Compute percentage of cooperation\n",
    "        coop_basin_size = (df['Classification'] == 'Cooperation').mean() * 100\n",
    "        cooperation_basin_sizes[mode] = round(coop_basin_size, 2)\n",
    "\n",
    "    # Compute absolute increase relative to \"no_information\"\n",
    "    baseline = cooperation_basin_sizes[\"no_information\"]\n",
    "    absolute_increase = {mode: round(size - baseline, 1) for mode, size in cooperation_basin_sizes.items()}\n",
    "\n",
    "    # Store results in a structured format\n",
    "    for mode in cooperation_basin_sizes:\n",
    "        results.append({\n",
    "            \"m\": m_value,\n",
    "            \"Information Condition\": mode,\n",
    "            \"Cooperation Basin Size (%)\": cooperation_basin_sizes[mode],\n",
    "            \"Absolute Increase (%)\": absolute_increase[mode] if mode != \"no_information\" else 0\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''avg policy for obs set'''\n",
    "\n",
    "\n",
    "#| export\n",
    "\n",
    "#average policy \n",
    "\n",
    "def average_policy_for_given_observation_set(X, O):\n",
    "    \"\"\"\n",
    "    Takes in the strategy wrt to complete state set as input and returns the average strategy wrt to the given observation set. For example Strategty with respect to state set might\n",
    "    have c, c, g = 0.2 and c, c, p = 0.8. But the observation set will be c, c, if only actions are observed. Then the average strategy for this observation set would be 0.5\n",
    "\n",
    "    \"\"\"\n",
    "    A, S, F = X.shape        # Number of layers, states, and features\n",
    "    _, _, num_obs = O.shape   # Number of observations\n",
    "\n",
    "    # Initialize the output matrix with zeros\n",
    "    output = np.zeros((A, num_obs, F))\n",
    "\n",
    "    for i in range(A):\n",
    "        for obs in range(num_obs):\n",
    "            current_mask = O[i, :, obs]  # Shape: (S,)\n",
    "\n",
    "            # Select rows from X where mask is 1\n",
    "            selected_X = X[i][current_mask == 1]  # Shape: (num_selected_states, F)\n",
    "            if selected_X.size > 0:\n",
    "                mean_vector = selected_X.mean(axis=0)  # Shape: (F,)\n",
    "            else:\n",
    "                mean_vector = np.zeros(F)  # Default to zero vector\n",
    "            output[i, obs, :] = mean_vector\n",
    "\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"random discrepances observed\"\"\"\n",
    "\n",
    "\n",
    "#degraded choice is false\n",
    "\n",
    "ecopg = EcologicalPublicGood(N=2, f=1.2, c=5, m=-4, qc=0.02, qr= 0.0001, degraded_choice=False)\n",
    "# ecopg = EcologicalPublicGood(N=2, f=1.2, c=5, m=-4, qc=0.2, qr= 0.1, degraded_choice=True)\n",
    "\n",
    "\n",
    "for mode in ['both_state_and_action_information', \n",
    "             'only_action_history_information', \n",
    "             'only_state_information', \n",
    "             'no_information'\n",
    "             ]:\n",
    "# for mode in ['both_state_and_action_information']:\n",
    "\n",
    "    # Initialize the information condition\n",
    "\n",
    "    information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "    mae = POstratAC(env=information_condition_instance, learning_rates=0.1, discount_factors=0.99)\n",
    "\n",
    "    # Data storage\n",
    "    \n",
    "    avg_coop_time_pairs = []\n",
    "    num_samples = 5\n",
    "    initial_conditions_list = lhs_sampling(mae.Q, num_samples, mae.N)\n",
    "\n",
    "    print(f\"\\nMode: {mode}\")\n",
    "\n",
    "    # Monte Carlo Simulations\n",
    "    for initial_condition in initial_conditions_list:\n",
    "    \n",
    "        # initial_condition = make_degraded_state_cooperation_probablity_zero(initial_condition, information_condition_instance.Oset[0]) #to make sure all of them start at the same position in the degraded state (shouldn't it)\n",
    "\n",
    "        xtraj, fixedpointreached = mae.trajectory(initial_condition, Tmax=10000, tolerance=1e-5)\n",
    "        final_point = xtraj[-1]\n",
    "        \n",
    "        avg_coop_across_states = get_average_cooperativeness(policy=final_point, obsdist=mae.obsdist(final_point), mode = mode, Oset = mae.env.Oset[0], exclude_degraded_state_for_average_cooperation=True)[0]\n",
    "        time_to_reach = xtraj.shape[0]\n",
    "\n",
    "        avg_coop_time_pairs.append((avg_coop_across_states, time_to_reach))\n",
    "\n",
    "    # Create DataFrame for processing\n",
    "    df = pd.DataFrame(avg_coop_time_pairs, columns=[\"AverageCooperation\", \"TimeToReach\"])\n",
    "    total_count = len(df)\n",
    "    # print(df)\n",
    "    \n",
    "    average_cooperation_across_initial_conditions = df['AverageCooperation'].agg('mean')\n",
    "    print(\"Mean Final Cooperation Across Initial Conditions \", np.round(average_cooperation_across_initial_conditions,2))\n",
    "\n",
    "\n",
    "    # Add a classification column\n",
    "    def classify(avg_coop):\n",
    "        if avg_coop < 0.1:\n",
    "            return \"Defection\"\n",
    "        elif avg_coop > 0.9:\n",
    "            return \"Cooperation\"\n",
    "        else:\n",
    "            return \"Mixed\"\n",
    "\n",
    "\n",
    "    df['Classification'] = df['AverageCooperation'].apply(classify)\n",
    "    # Reporting unique entries\n",
    "    # Overall Summary\n",
    "    summary = df.groupby('Classification')['TimeToReach'].agg(\n",
    "        MedianTimetoReach='median',\n",
    "        Percentage= lambda x: round((len(x) / total_count) * 100,1)\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
