{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "998aa58c-c1c4-41fe-9f34-1a91133c87cd",
   "metadata": {},
   "source": [
    "\n",
    "# Information Conditions - Environmental State and Action Histories\n",
    "\n",
    "> Plot learning trajectories under different information conditions of the Ecological Public Goods Game. a) Only environmental state history observable, b) only action history is observable, c) both environmental state and action histories are observable d) No information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a99f35f-d960-411d-868c-4d789c9e6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from pyCRLD.Environments.SocialDilemma import SocialDilemma\n",
    "from pyCRLD.Environments.EcologicalPublicGood import EcologicalPublicGood\n",
    "\n",
    "from pyCRLD.Agents.StrategyActorCritic import stratAC\n",
    "from pyCRLD.Agents.POStrategyActorCritic import POstratAC\n",
    "\n",
    "\n",
    "from pyCRLD.Utils import FlowPlot as fp\n",
    "from fastcore.utils import *\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "from pyCRLD.Environments.HistoryEmbedding import HistoryEmbedded\n",
    "\n",
    "from nbdev.showdoc import show_doc\n",
    "from scipy.stats import kstest\n",
    "\n",
    "from scipy.stats import qmc\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "\n",
    "global_seed = 42\n",
    "np.random.seed(global_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc6b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#generate \n",
    "def generate_action_history_observation_set(stateset, number_of_agents):\n",
    "    action_histories = [state[:3] for state in stateset]\n",
    "    unique_action_histories = sorted(list(set(action_histories)))\n",
    "    Oset = [unique_action_histories.copy() for _ in range(number_of_agents)]\n",
    "    return Oset\n",
    "\n",
    "\n",
    "def generate_state_observation_set(stateset, number_of_agents):\n",
    "    state_histories = [state[4:] for state in stateset]\n",
    "    unique_state_histories = sorted(list(set(state_histories)))\n",
    "    Oset = [unique_state_histories.copy() for _ in range(number_of_agents)]\n",
    "    return Oset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcbb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#information conditions class\n",
    "class Information_Conditions(HistoryEmbedded):\n",
    "    def __init__(self, ecopg , mode):\n",
    "\n",
    "\n",
    "        super().__init__(ecopg, h=(1, 1, 1))\n",
    "\n",
    "        self.mode = mode\n",
    "        self.configure_information_condition()\n",
    "\n",
    "    def configure_information_condition(self):\n",
    "        \"\"\"\n",
    "        Set the observation mode and configure the observation tensor, Oset, and other properties.\n",
    "        Modes: 'state', 'action', 'none', 'state+action'\n",
    "        \"\"\"\n",
    "        if self.mode == \"only_state_information\":\n",
    "            self._configure_state()\n",
    "        elif self.mode == \"only_action_history_information\":\n",
    "            self._configure_action()\n",
    "        elif self.mode == \"no_information\":\n",
    "            self._configure_none()\n",
    "        elif self.mode == \"both_state_and_action_information\":\n",
    "            self._configure_state_action()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode..\")\n",
    "        # self._print_configuration()\n",
    "\n",
    "        self.Q = self.O.shape[2]\n",
    "\n",
    "    def _configure_state(self):\n",
    "        def generate_state_tensor(state_set, observation_set):\n",
    "            state_tensor = np.zeros((2, len(state_set), len(observation_set)), dtype=int)\n",
    "            for i in range(2):\n",
    "                for j, state in enumerate(state_set):\n",
    "                    for k, observation in enumerate(observation_set):\n",
    "                        if state.endswith(observation):\n",
    "                            state_tensor[i, j, k] = 1\n",
    "            return state_tensor\n",
    "        \n",
    "        \n",
    "        self.Oset = generate_state_observation_set(self.Sset, 2)\n",
    "\n",
    "        self.O = generate_state_tensor(\n",
    "            self.Sset,  self.Oset[0])\n",
    "        \n",
    "\n",
    "    def _configure_action(self):\n",
    "        def generate_action_tensor(state_set, action_set):\n",
    "            action_tensor = np.zeros((2, len(state_set), len(action_set)), dtype=int)\n",
    "            for i in range(2):  \n",
    "                for j, state in enumerate(state_set):\n",
    "                    for k, action in enumerate(action_set):\n",
    "                        if action[:3] == state[:3]:\n",
    "                            action_tensor[i, j, k] = 1\n",
    "            return action_tensor\n",
    "\n",
    "        self.Oset = generate_action_history_observation_set(self.Sset, self.N)\n",
    "        self.O = generate_action_tensor(self.Sset, self.Oset[0])\n",
    "\n",
    "    def _configure_none(self):\n",
    "        def generate_none_tensor():\n",
    "            return np.ones((2, 8, 1), dtype=int)\n",
    "\n",
    "        self.O = generate_none_tensor()\n",
    "        self.Oset = [['.'], ['.']]\n",
    "\n",
    "    def _configure_state_action(self):\n",
    "        # This assumes the default state+action information in `ecopg_with_history`\n",
    "        pass\n",
    "          # No modification needed; default setup already uses state+action information.\n",
    "\n",
    "\n",
    "    def _print_configuration(self):\n",
    "        print(f\"Mode: {self.mode}\")\n",
    "        # print(\"Observation Tensor:\\n\", self.O)\n",
    "        # print(\"Observation Set:\", self.Oset)\n",
    "        # print(\"O shape\", self.O.shape)\n",
    "        # print(\"Q shape\", self.Q)\n",
    "\n",
    "        print(\"------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995eece",
   "metadata": {},
   "source": [
    "### Experiments and Observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd8320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=4, suppress= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4c48c",
   "metadata": {},
   "source": [
    "Complete state and action information - Monte Carlo analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5331db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lhs_sampling(no_of_states, number_of_samples, agents):\n",
    "    global global_seed\n",
    "    sampler = qmc.LatinHypercube(d=no_of_states, seed = global_seed)\n",
    "\n",
    "    # Sampling for each agent and stacking similar result lists\n",
    "    lhs_random_samples_list = sampler.random(number_of_samples)\n",
    "    result = [np.stack((random_samples, 1 - random_samples), axis=-1) for random_samples in lhs_random_samples_list]\n",
    "    cross_product = [np.stack((x, y), axis=0) for x, y in it.combinations_with_replacement(result, agents)]\n",
    "\n",
    "    return cross_product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8679fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_degraded_state_cooperation_probablity_zero(initial_condition, Oset):\n",
    "\n",
    "    degraded_mask = jnp.array(['g' in label for label in Oset])\n",
    "    initial_condition[:, degraded_mask, 0] = 0\n",
    "    initial_condition[:, degraded_mask, 1] = 1\n",
    "\n",
    "    return initial_condition\n",
    "\n",
    "def make_degraded_state_cooperation_probablity_one(initial_condition, Oset):\n",
    "\n",
    "    degraded_mask = jnp.array(['g' in label for label in Oset])\n",
    "    initial_condition[:, degraded_mask, 0] = 1\n",
    "    initial_condition[:, degraded_mask, 1] = 0\n",
    "\n",
    "    return initial_condition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef36b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_degraded_states_from_obsdist(obsdist, Oset):\n",
    "   \n",
    "        # Exclude degraded states from the observation distribution\n",
    "\n",
    "    degraded_mask = jnp.array(['g' in label for label in Oset])\n",
    "    obsdist = jnp.where(degraded_mask, 0, obsdist)\n",
    "\n",
    "    # Normalize rows to ensure sum of probabilities is 1\n",
    "    # row_sums = jnp.sum(obsdist, axis=1, keepdims=True)\n",
    "    # obsdist_without_degraded_state = jnp.where(row_sums > 0, obsdist / row_sums, obsdist)  # Avoid division by zero\n",
    "\n",
    "    return obsdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "936c087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_degraded_states_from_obsdist_and_normalise(obsdist, Oset):\n",
    "   \n",
    "        # Exclude degraded states from the observation distribution\n",
    "\n",
    "    degraded_mask = jnp.array(['g' in label for label in Oset])\n",
    "    obsdist = jnp.where(degraded_mask, 0, obsdist)\n",
    "\n",
    "    # Normalize rows to ensure sum of probabilities is 1\n",
    "    row_sums = jnp.sum(obsdist, axis=1, keepdims=True)\n",
    "    obsdist_without_degraded_state = jnp.where(row_sums > 0, obsdist / row_sums, obsdist)  # Avoid division by zero\n",
    "\n",
    "    return obsdist_without_degraded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8262692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cooperativeness(policy, obsdist, mode, Oset, exclude_degraded_state_for_average_cooperation):\n",
    "    \n",
    "    if exclude_degraded_state_for_average_cooperation:\n",
    "        if mode == 'only_state_information' or mode == 'both_state_and_action_information':\n",
    "            obsdist = exclude_degraded_states_from_obsdist(obsdist, Oset)\n",
    "\n",
    "        \n",
    "    policy_cooperation_probabilities = policy[:,:, 0]\n",
    "    agent_index, state_index = [0, 1]\n",
    "\n",
    "    average_cooperation_for_each_agent = jnp.einsum(policy_cooperation_probabilities, [agent_index, state_index], obsdist, [agent_index, state_index], [agent_index])\n",
    "    \n",
    "    return average_cooperation_for_each_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eaa943",
   "metadata": {},
   "source": [
    "Extracting Final Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb5363",
   "metadata": {},
   "source": [
    "1. State information activb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f5304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_for_initial_condition(mae, mode, initial_condition, \n",
    "                                         exclude_degraded_state_for_average_cooperation):\n",
    "    \"\"\"\n",
    "    Runs a single Monte Carlo simulation and returns the average cooperation and time-to-reach.\n",
    "\n",
    "    Parameters:\n",
    "        mae: The POstratAC instance (learning agent).\n",
    "        information_condition_instance: The instance of Information_Conditions.\n",
    "        initial_condition: The sampled initial condition for the simulation.\n",
    "        initial_cooperation_in_degraded_state (int): If 0, cooperation in degraded state is set to zero;\n",
    "                                                     if 1, it is set to one; otherwise, no changes.\n",
    "        include_degraded_state_for_average_cooperation (bool): Whether to include the degraded state in average cooperation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average cooperation, time-to-reach)\n",
    "    \"\"\"\n",
    "\n",
    "    xtraj, fixedpointreached = mae.trajectory(initial_condition, Tmax=10000, tolerance=1e-5)\n",
    "    final_point = xtraj[-1]\n",
    "\n",
    "    avg_coop_across_states = get_average_cooperativeness(\n",
    "        policy=final_point, \n",
    "        obsdist=mae.obsdist(final_point), \n",
    "        mode=mode, \n",
    "        Oset=mae.env.Oset[0],\n",
    "        exclude_degraded_state_for_average_cooperation = exclude_degraded_state_for_average_cooperation\n",
    "    )[0]  #we're only considiering agent i\n",
    "\n",
    "    time_to_reach = xtraj.shape[0]\n",
    "\n",
    "    return avg_coop_across_states, time_to_reach\n",
    "\n",
    "\n",
    "def run_simulation_across_conditions(mae, mode, num_samples, \n",
    "                                     exclude_degraded_state_for_average_cooperation):\n",
    "    \"\"\"\n",
    "    Runs Monte Carlo simulations across multiple initial conditions.\n",
    "\n",
    "    Parameters:\n",
    "        mae: The POstratAC instance (learning agent).\n",
    "        information_condition_instance: The instance of Information_Conditions.\n",
    "        num_samples (int): Number of initial conditions to sample.\n",
    "        initial_cooperation_in_degraded_state (int): If 0, cooperation in degraded state is set to zero;\n",
    "                                                     if 1, it is set to one; otherwise, no changes.\n",
    "        include_degraded_state_for_average_cooperation (bool): Whether to include the degraded state in average cooperation.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (average cooperation, time-to-reach) tuples.\n",
    "    \"\"\"\n",
    "    avg_coop_time_pairs = []\n",
    "    initial_conditions_list = lhs_sampling(mae.Q, num_samples, mae.N)\n",
    "\n",
    "    for initial_condition in initial_conditions_list:\n",
    "        result = run_simulation_for_initial_condition(\n",
    "            mae, mode, initial_condition,\n",
    "             exclude_degraded_state_for_average_cooperation\n",
    "        )\n",
    "        avg_coop_time_pairs.append(result)\n",
    "\n",
    "    return avg_coop_time_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdfd0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_four_conditions(num_samples=5, degraded_choice = True, m_value = -4, discount_factor = 0.9, exclude_degraded_state_for_average_cooperation = True):\n",
    "    \"\"\"\n",
    "    Runs simulations for different information conditions and outputs \n",
    "    the results for each condition.\n",
    "    \n",
    "    Parameters:\n",
    "        ecopg (EcologicalPublicGood): An instance of the ecological public good model.\n",
    "        num_samples (int): Number of initial conditions to sample.\n",
    "        Tmax (int): Maximum time steps for trajectory simulation.\n",
    "        tolerance (float): Convergence tolerance for fixed point detection.\n",
    "        \n",
    "    Returns:\n",
    "        None (prints the output summaries for each information condition)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    information_modes = [\n",
    "        'both_state_and_action_information', \n",
    "        'only_action_history_information', \n",
    "        'only_state_information', \n",
    "        'no_information'\n",
    "    ]\n",
    "\n",
    "    basin_of_attraction_and_avg_cooperation_results = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    ecopg = EcologicalPublicGood(N=2,\n",
    "                                 f=1.2, \n",
    "                                 c=5, \n",
    "                                 m= m_value,\n",
    "                                 qc=0.02, \n",
    "                                 qr= 0.0001, \n",
    "                                 degraded_choice = degraded_choice)\n",
    "\n",
    "\n",
    "    for mode in information_modes:\n",
    "        # Initialize the information condition\n",
    "        information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "        mae = POstratAC(env=information_condition_instance, learning_rates=0.1, discount_factors= discount_factor)\n",
    "\n",
    "        # Data storage\n",
    "\n",
    "        # print(f\"\\nMode: {mode}\")\n",
    "\n",
    "        avg_coop_time_pairs = run_simulation_across_conditions(\n",
    "            mae = mae, \n",
    "            mode = mode,\n",
    "            num_samples = num_samples, \n",
    "            exclude_degraded_state_for_average_cooperation = exclude_degraded_state_for_average_cooperation\n",
    "        )\n",
    "\n",
    "        # Create DataFrame for processing\n",
    "        df = pd.DataFrame(avg_coop_time_pairs, columns=[\"AverageCooperation\", \"TimeToReach\"])\n",
    "        total_count = len(df)\n",
    "        # print(df)\n",
    "\n",
    "\n",
    "        average_cooperation_across_initial_conditions = df['AverageCooperation'].agg('mean')\n",
    "        # print(\"Mean Final Cooperation Across Initial Conditions:\", np.round(average_cooperation_across_initial_conditions,2))\n",
    "\n",
    "        # Classification function\n",
    "    \n",
    "\n",
    "        df['Classification'] = df['AverageCooperation'].apply(lambda x: \"Defection\" if x < 0.1 else \"Cooperation\" if x > 0.9 else \"Mixed\" )\n",
    "\n",
    "        # Summary statistics\n",
    "        basin_of_attraction_size = df.groupby('Classification')['TimeToReach'].agg(\n",
    "            MedianTimetoReach='median',\n",
    "            Percentage=lambda x: round((len(x) / total_count) * 100, 1)\n",
    "        ).reset_index()\n",
    "\n",
    "        basin_of_attraction_and_avg_cooperation_results[mode] = {\n",
    "        \"average_cooperation\": np.round(average_cooperation_across_initial_conditions, 3),\n",
    "        \"basin_of_attraction_size\": basin_of_attraction_size\n",
    "        }\n",
    "\n",
    "\n",
    "    return basin_of_attraction_and_avg_cooperation_results\n",
    "    \n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac40af71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'both_state_and_action_information': {'average_cooperation': 0.067,\n",
       "  'basin_of_attraction_size':   Classification  MedianTimetoReach  Percentage\n",
       "  0    Cooperation               70.0         6.7\n",
       "  1      Defection            10000.0        93.3},\n",
       " 'only_action_history_information': {'average_cooperation': 0.187,\n",
       "  'basin_of_attraction_size':   Classification  MedianTimetoReach  Percentage\n",
       "  0      Defection            10000.0        46.7\n",
       "  1          Mixed            10000.0        53.3},\n",
       " 'only_state_information': {'average_cooperation': 0.0,\n",
       "  'basin_of_attraction_size':   Classification  MedianTimetoReach  Percentage\n",
       "  0      Defection            10000.0       100.0},\n",
       " 'no_information': {'average_cooperation': 0.052,\n",
       "  'basin_of_attraction_size':   Classification  MedianTimetoReach  Percentage\n",
       "  0      Defection            10000.0        86.7\n",
       "  1          Mixed            10000.0        13.3}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_four_conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_iteration_discount_factors_m_vals = []\n",
    "\n",
    "for m_value in range(0, -7, -2):\n",
    "    # Iterate over discount factor from 0.9 to 0.99 in steps of 0.02\n",
    "    for discount_factor in np.arange(0.9, 0.999, 0.3):\n",
    "        result = compare_four_conditions(m_value=m_value, discount_factor=discount_factor)\n",
    "        \n",
    "        results_iteration_discount_factors_m_vals.append({\"m\": m_value, \"discount_factor\": discount_factor, \"result\": result})\n",
    "\n",
    "\n",
    "# Print or store results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
