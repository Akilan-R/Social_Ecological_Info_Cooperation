{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095b105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from information_conditions import Information_Conditions\n",
    "from base_ecopg import *\n",
    "from helper_functions import *\n",
    "from simulation_and_results_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38583a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8, suppress=True, linewidth=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b957bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_degraded_state_policies_both_state_and_action(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "\n",
    "        for i in [0, 2, 4, 6]:\n",
    "            x = np.random.rand()\n",
    "            strategy_propserous_and_degraded_state.insert(i, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n",
    "\n",
    "def add_degraded_state_policies_only_state(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "        x = np.random.rand()\n",
    "        strategy_propserous_and_degraded_state.insert(0, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52af6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_from_strategy(agent_1_strategy, agent_2_strategy):\n",
    "\n",
    "\n",
    "    agent_1_strategy = [[x, 1-x] for x in agent_1_strategy]\n",
    "    agent_2_strategy = [[x, 1-x] for x in agent_2_strategy]\n",
    "\n",
    "    policy = np.array([agent_1_strategy, agent_2_strategy])\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def epsilon_noise_for_strategy(determinstic_strategy, epsilon):\n",
    "    #to avoid pure determinstic strategies, we remove e from 0  \n",
    "    strategy_with_epsilon = [x + epsilon if x == 0 else x - epsilon if x == 1 else x for x in determinstic_strategy]\n",
    "    return strategy_with_epsilon\n",
    "\n",
    "\n",
    "\n",
    "def create_policy_with_noise_from_deterministic_strategy(agent_1_strategy, agent_2_strategy, epsilon):\n",
    "\n",
    "    \n",
    "    determinstic_policy = create_policy_from_strategy(agent_1_strategy, agent_2_strategy)\n",
    "    \n",
    "    \n",
    "    agent_1_strategy_with_noise = epsilon_noise_for_strategy(agent_1_strategy, epsilon)\n",
    "    agent_2_strategy_with_noise = epsilon_noise_for_strategy(agent_2_strategy, epsilon)\n",
    "\n",
    "    noisy_policy = create_policy_from_strategy(agent_1_strategy_with_noise, agent_2_strategy_with_noise)\n",
    "\n",
    "    return noisy_policy, determinstic_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58d1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy_obs(policy, mode):\n",
    "\n",
    "    mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    # mae_ecopg.has_last_statdist = False\n",
    "    # mae_ecopg._last_statedist = None\n",
    "\n",
    "    Vio = mae_ecopg.Vio(policy)\n",
    "    obsdist = mae_ecopg.obsdist(policy)\n",
    "\n",
    "    # print(Vio, \"Vio\")\n",
    "    # print(obsdist, \"obsdist\")\n",
    "\n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vio = Vio[:,1::2]\n",
    "        obsdist = obsdist[:,1::2]\n",
    "        obsdist = obsdist/obsdist.sum(axis = 1)\n",
    "\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "    # print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "    return avg_value_agent_1, avg_value_agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52a781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy_state_mae_ecopg(policy, mode):\n",
    "\n",
    "    mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    Vis = mae_ecopg.Vis(policy)\n",
    "    # mae_ecopg.has_last_statdist = False\n",
    "    # mae_ecopg._last_statedist = None\n",
    "    # state_dist = mae_ecopg._numpyPs(policy)\n",
    "    \n",
    "    state_dist = mae_ecopg.Ps(policy)\n",
    "    \n",
    "    # E = 0.01\n",
    "    # state_dist = np.where(state_dist == 0, E, state_dist)\n",
    "\n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "\n",
    "    state_dist = state_dist/np.sum(state_dist)\n",
    "\n",
    "    # print(Vis, \"vis\")\n",
    "    # print(state_dist, \"state dist\")\n",
    "\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "    # print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "    return avg_value_agent_1, avg_value_agent_2\n",
    "\n",
    "\n",
    "\n",
    "def calculate_avg_value_given_policy_state_test(policy, mode, mae):\n",
    "\n",
    "    Vis = mae.Vis(policy)\n",
    "    state_dist = mae.Ps(policy)\n",
    "\n",
    "    # print(Vis, \"vis\")\n",
    "    # print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "\n",
    "\n",
    "    return  avg_value_agent_1, avg_value_agent_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db3dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players(mode):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    # E = 0.05\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 , 0], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1, 0], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68a3669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players_with_epsilon(mode, epsilon):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 - epsilon , 0 + epsilon], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1 - epsilon , 0 + epsilon], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3200d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_matrix_to_excel(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    '''p1 rewards pg 1. p2 rewards pg 2. rows -> p1 strategies, cols -> p2 strategies in both cases '''\n",
    "    p1_reward_matrix = pd.DataFrame(p1_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "    p2_reward_matrix = pd.DataFrame(p2_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "\n",
    "    with pd.ExcelWriter('reward_matrix_only_action_2.xlsx') as excel_file:\n",
    "        p1_reward_matrix.to_excel(excel_file, sheet_name='p1', index=True, header=True)\n",
    "        p2_reward_matrix.to_excel(excel_file, sheet_name='p2', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d7dd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pure_strategy_nash_equilibria(payoffs_row, payoffs_col):\n",
    "    game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "    result = pygambit.nash.enumpure_solve(game)\n",
    "    return result.equilibria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb89d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nstrategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\\nstrategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\\n\\nprint(strategy_set_p1_only_action_array)\\n\\nfor eq in result.equilibria:                       # each equilibrium profile\\n    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\\n\\n    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\\n    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\\n    # float_profile = np.array() # iterate over (strategy, prob\\n\\n    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\\n    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\\n\\n    print('------')\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_and_print_nash_equilibria_results(nash_equlibria_result, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    number_of_nash_equilbria = len(nash_equlibria_result)\n",
    "    print(\"Total number of Nash \", number_of_nash_equilbria)\n",
    "\n",
    "    '''better readable format of the strategies'''\n",
    "    strategies_p1_array = np.array(list(strategy_set_p1.keys())) #just the list of strategies\n",
    "    strategies_p2_array = np.array(list(strategy_set_p2.keys()))\n",
    "\n",
    "    for eq in nash_equlibria_result:                       # each equilibrium profile\n",
    "        strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]    #converting mixed strategis to float\n",
    "\n",
    "        p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool) #boolean conversion - for strategy in pure nash probability =  1, for strategy not in pure nash, probability = 0\n",
    "        p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "        # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "        p1_nash_strategy = strategies_p1_array[p1_active_strategy]\n",
    "        p2_nash_strategy =  strategies_p2_array[p2_active_strategy]\n",
    "\n",
    "        \n",
    "        print('P1:' , p1_nash_strategy)   #will only print the strategy found in nash\n",
    "        print('P2:' , p2_nash_strategy)\n",
    "\n",
    "        print('------')\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "strategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\n",
    "strategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "print(strategy_set_p1_only_action_array)\n",
    "\n",
    "for eq in result.equilibria:                       # each equilibrium profile\n",
    "    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\n",
    "\n",
    "    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\n",
    "    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "    # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\n",
    "    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\n",
    "\n",
    "    print('------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c8ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode):\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "    \n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_obs(policy, mode)\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_mae_ecopg(policy, mode)\n",
    "\n",
    "            # print(value_p1, value_p2 ,\"value p1, p2\")\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metagame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode, mae):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_test(policy, mode, mae)\n",
    "\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "924aefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pure_nash_equilibria_all_deterministic_strategies_mode(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "    p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "    # p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "\n",
    "    # print(\"p1: \\n\", p1_reward_matrix, \"\\n p2: \\n\", p2_reward_matrix)\n",
    "    \n",
    "    nash_equlibrium_result = calculate_pure_strategy_nash_equilibria(p1_reward_matrix, p2_reward_matrix)\n",
    "    process_and_print_nash_equilibria_results(nash_equlibrium_result, strategy_set_p1, strategy_set_p2)\n",
    "\n",
    "def get_pure_nash_equilibria_all_deterministic_strategies_mode_obs(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "    # p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "    p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "\n",
    "    # print(\"p1: \\n\", p1_reward_matrix, \"\\n p2: \\n\", p2_reward_matrix)\n",
    "    \n",
    "    nash_equlibrium_result = calculate_pure_strategy_nash_equilibria(p1_reward_matrix, p2_reward_matrix)\n",
    "    process_and_print_nash_equilibria_results(nash_equlibrium_result, strategy_set_p1, strategy_set_p2)\n",
    "\n",
    "def get_pure_nash_equilibria_all_deterministic_strategies_mode_test(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "    p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode)\n",
    "    # p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "\n",
    "    # print(\"p1: \\n\", p1_reward_matrix, \"\\n p2: \\n\", p2_reward_matrix)\n",
    "    \n",
    "    nash_equlibrium_result = calculate_pure_strategy_nash_equilibria(p1_reward_matrix, p2_reward_matrix)\n",
    "    process_and_print_nash_equilibria_results(nash_equlibrium_result, strategy_set_p1, strategy_set_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1166f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===  mode: none ===\n",
      "Total number of Nash  1\n",
      "P1: ['[0.99]']\n",
      "P2: ['[0.99]']\n",
      "------\n",
      "===  mode: ecological ===\n",
      "Total number of Nash  2\n",
      "P1: ['[0.99]']\n",
      "P2: ['[0.99]']\n",
      "------\n",
      "P1: ['[0.01]']\n",
      "P2: ['[0.01]']\n",
      "------\n",
      "===  mode: social ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mecological\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msocial\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcomplete\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m===  mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mget_pure_nash_equilibria_all_deterministic_strategies_mode_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mget_pure_nash_equilibria_all_deterministic_strategies_mode_obs\u001b[39m\u001b[34m(mode)\u001b[39m\n\u001b[32m     12\u001b[39m strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m p1_reward_matrix, p2_reward_matrix = \u001b[43mmetagame_reward_matrix_obs_ecopg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy_set_p1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_set_p2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# print(\"p1: \\n\", p1_reward_matrix, \"\\n p2: \\n\", p2_reward_matrix)\u001b[39;00m\n\u001b[32m     18\u001b[39m nash_equlibrium_result = calculate_pure_strategy_nash_equilibria(p1_reward_matrix, p2_reward_matrix)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mmetagame_reward_matrix_obs_ecopg\u001b[39m\u001b[34m(strategy_set_p1, strategy_set_p2, mode)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, p2_strategy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(strategy_set_p2.values()):\n\u001b[32m      9\u001b[39m     policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     value_p1, value_p2 = \u001b[43mcalculate_avg_value_given_policy_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     p1_reward_matrix[i,j] = value_p1\n\u001b[32m     12\u001b[39m     p2_reward_matrix[i,j] = value_p2\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcalculate_avg_value_given_policy_obs\u001b[39m\u001b[34m(policy, mode)\u001b[39m\n\u001b[32m      3\u001b[39m mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# mae_ecopg.has_last_statdist = False\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# mae_ecopg._last_statedist = None\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m Vio = \u001b[43mmae_ecopg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m obsdist = mae_ecopg.obsdist(policy)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# print(Vio, \"Vio\")\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# print(obsdist, \"obsdist\")\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\pjit.py:339\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    335\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    338\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m  pgle_profiler) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    342\u001b[39m     executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr.effects,\n\u001b[32m    343\u001b[39m     jaxpr.consts, jit_info.abstracted_axes,\n\u001b[32m    344\u001b[39m     pgle_profiler)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\pjit.py:194\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    193\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m   out_flat, compiled, profiler = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    196\u001b[39m   out_flat = pjit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\pjit.py:1659\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1647\u001b[39m compiler_options_kvs = compiler_options_kvs + \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options.items())\n\u001b[32m   1648\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1649\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1650\u001b[39m compiled = \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2448\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options)\u001b[39m\n\u001b[32m   2446\u001b[39m compiler_options_kvs = \u001b[38;5;28mself\u001b[39m._compiler_options_kvs + t_compiler_options\n\u001b[32m   2447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs:\n\u001b[32m-> \u001b[39m\u001b[32m2448\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2449\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2450\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2451\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2967\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2964\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   2966\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2967\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2973\u001b[39m orig_out_shardings = out_shardings\n\u001b[32m   2975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2758\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2750\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2751\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2752\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2753\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2755\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2756\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2757\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2758\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\compiler.py:470\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    469\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\compiler.py:687\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    679\u001b[39m     backend: xc.Client,\n\u001b[32m    680\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    684\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    685\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    686\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m   executable = \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    691\u001b[39m   _cache_write(\n\u001b[32m    692\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    693\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\profiler.py:334\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    333\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\jax\\_src\\compiler.py:321\u001b[39m, in \u001b[36mbackend_compile\u001b[39m\u001b[34m(backend, module, options, host_callbacks)\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile(\n\u001b[32m    316\u001b[39m         built_c, compile_options=options, host_callbacks=host_callbacks\n\u001b[32m    317\u001b[39m     )\n\u001b[32m    318\u001b[39m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    319\u001b[39m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    320\u001b[39m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m xc.XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    323\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for mode in ['none', 'ecological', 'social', 'complete']:\n",
    "    print(f\"===  mode: {mode} ===\")\n",
    "    get_pure_nash_equilibria_all_deterministic_strategies_mode_obs(mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9373e81",
   "metadata": {},
   "source": [
    "###Nash Results\n",
    "\n",
    "\n",
    "===  mode: complete ===\n",
    "Total number of Nash  4\n",
    "P1: ['[0.99, 0.99, 0.99, 0.99]']\n",
    "P2: ['[0.99, 0.99, 0.99, 0.99]']\n",
    "------\n",
    "P1: ['[0.99, 0.01, 0.01, 0.99]']\n",
    "P2: ['[0.99, 0.01, 0.01, 0.99]']\n",
    "------\n",
    "P1: ['[0.01, 0.01, 0.01, 0.99]']\n",
    "P2: ['[0.01, 0.01, 0.01, 0.99]']\n",
    "------\n",
    "P1: ['[0.01, 0.01, 0.01, 0.01]']\n",
    "P2: ['[0.01, 0.01, 0.01, 0.01]']\n",
    "------\n",
    "===  mode: ecological ===\n",
    "Total number of Nash  2\n",
    "P1: ['[0.99]']\n",
    "P2: ['[0.99]']\n",
    "------\n",
    "P1: ['[0.01]']\n",
    "P2: ['[0.01]']\n",
    "------\n",
    "===  mode: social ===\n",
    "Total number of Nash  3\n",
    "P1: ['[0.99, 0.99, 0.99, 0.99]']\n",
    "P2: ['[0.99, 0.99, 0.99, 0.99]']\n",
    "------\n",
    "P1: ['[0.99, 0.01, 0.01, 0.99]']\n",
    "P2: ['[0.99, 0.01, 0.01, 0.99]']\n",
    "------\n",
    "P1: ['[0.01, 0.01, 0.01, 0.99]']\n",
    "P2: ['[0.01, 0.01, 0.01, 0.99]']\n",
    "------\n",
    "===  mode: none ===\n",
    "Total number of Nash  1\n",
    "P1: ['[0.99]']\n",
    "P2: ['[0.99]']\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62309c48",
   "metadata": {},
   "source": [
    "Qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd252aca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "metagame_reward_matrix_obs_ecopg() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n\u001b[32m     13\u001b[39m mae_socdi = stratAC(env=env_memo1pd, learning_rates=\u001b[32m0.1\u001b[39m, discount_factors= \u001b[32m0.99\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m p1_reward_matrix_only_state, p2_reward_matrix_only_state = \u001b[43mmetagame_reward_matrix_obs_ecopg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy_set_p1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_set_p2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmae_socdi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m payoffs_row = p1_reward_matrix_only_state\n\u001b[32m     18\u001b[39m payoffs_col  = p2_reward_matrix_only_state\n",
      "\u001b[31mTypeError\u001b[39m: metagame_reward_matrix_obs_ecopg() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "'''Memo1pd testing for understanding '''\n",
    "\n",
    "\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "\n",
    "\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "\n",
    "\n",
    "\n",
    "process_and_print_nash_equilibria_results_test(result.equilibria, strategy_set_p1, strategy_set_p2, mode, mae_socdi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c507c",
   "metadata": {},
   "source": [
    "Reward prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551a154",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RPE_stability_check() missing 1 required positional argument: 'mae'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# print(\"==\", mode, \"===\")\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(policies[results])\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mcheck_policy_for_stability_RPE_all_socdi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mcheck_policy_for_stability_RPE_all_socdi\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,\u001b[32m2\u001b[39m)\n\u001b[32m     23\u001b[39m policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) \u001b[38;5;28;01mfor\u001b[39;00m p1_strategy, p2_strategy \u001b[38;5;129;01min\u001b[39;00m strategy_combinations])\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRPE_stability_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# print(\"==\", mode, \"===\")\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: RPE_stability_check() missing 1 required positional argument: 'mae'"
     ]
    }
   ],
   "source": [
    "def RPE_stability_check(policy, mae):\n",
    "\n",
    "    prob_of_coop = policy[:, :, 0]\n",
    "    # print(prob_of_coop)\n",
    "\n",
    "    reward_prediction_error = mae.RPEisa(policy)\n",
    "    # print(reward_prediction_error)\n",
    "    argmin_Q = np.argmin(reward_prediction_error, axis = 2) #if erro i s -.5 theb the poliviy id \n",
    "    # print(argmin_Q)\n",
    "\n",
    "    stability = np.all(prob_of_coop == argmin_Q)\n",
    "    \n",
    "    return stability\n",
    "\n",
    "\n",
    "\n",
    "def check_policy_for_stability_RPE_all_socdi():\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players('social')\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    mae = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "\n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    results = list(map(RPE_stability_check, policies))\n",
    "    print(results)\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(policies[results])\n",
    "\n",
    "\n",
    "check_policy_for_stability_RPE_all_socdi()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
