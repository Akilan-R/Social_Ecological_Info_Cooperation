{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095b105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from information_conditions import Information_Conditions\n",
    "from base_ecopg import *\n",
    "from helper_functions import *\n",
    "from simulation_and_results_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38583a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8, suppress=True, linewidth=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b957bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_degraded_state_policies_both_state_and_action(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "\n",
    "        for i in [0, 2, 4, 6]:\n",
    "            x = np.random.rand()\n",
    "            strategy_propserous_and_degraded_state.insert(i, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n",
    "\n",
    "def add_degraded_state_policies_only_state(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "        x = np.random.rand()\n",
    "        strategy_propserous_and_degraded_state.insert(0, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52af6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_from_strategy(agent_1_strategy, agent_2_strategy):\n",
    "\n",
    "\n",
    "    agent_1_strategy = [[x, 1-x] for x in agent_1_strategy]\n",
    "    agent_2_strategy = [[x, 1-x] for x in agent_2_strategy]\n",
    "\n",
    "    policy = np.array([agent_1_strategy, agent_2_strategy])\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def epsilon_noise_for_strategy(determinstic_strategy, epsilon):\n",
    "    #to avoid pure determinstic strategies, we remove e from 0  \n",
    "    strategy_with_epsilon = [x + epsilon if x == 0 else x - epsilon if x == 1 else x for x in determinstic_strategy]\n",
    "    return strategy_with_epsilon\n",
    "\n",
    "\n",
    "\n",
    "def create_policy_with_noise_from_deterministic_strategy(agent_1_strategy, agent_2_strategy, epsilon):\n",
    "\n",
    "    \n",
    "    determinstic_policy = create_policy_from_strategy(agent_1_strategy, agent_2_strategy)\n",
    "    \n",
    "    \n",
    "    agent_1_strategy_with_noise = epsilon_noise_for_strategy(agent_1_strategy, epsilon)\n",
    "    agent_2_strategy_with_noise = epsilon_noise_for_strategy(agent_2_strategy, epsilon)\n",
    "\n",
    "    noisy_policy = create_policy_from_strategy(agent_1_strategy_with_noise, agent_2_strategy_with_noise)\n",
    "\n",
    "    return noisy_policy, determinstic_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db3dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players(mode):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    # E = 0.05\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 , 0], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1, 0], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68a3669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players_with_epsilon(mode, epsilon):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 - epsilon , 0 + epsilon], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1 - epsilon , 0 + epsilon], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62309c48",
   "metadata": {},
   "source": [
    "Qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66cfa859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qioa \n",
      " [[[-298.5296 -298.52  ]]\n",
      "\n",
      " [[-298.5296 -298.52  ]]]\n",
      "Qi \n",
      " [[-298.5201]\n",
      " [-298.5201]]\n",
      "Qi_reverse \n",
      " [[-298.5295]\n",
      " [-298.5295]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_policy_for_stability_Q_values(determinstic_policy, mode):\n",
    "    #works only for determentsic strtagues\n",
    "    mae = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "\n",
    "    mae.Ps(determinstic_policy)\n",
    "    Qioa = mae.Qioa(determinstic_policy)\n",
    "    obsdist = mae.obsdist(determinstic_policy)\n",
    "\n",
    "    # print(determinstic_policy)\n",
    "\n",
    "    E = 0\n",
    "    \n",
    "    determinstic_policy_eps = np.where(determinstic_policy == 0, E, determinstic_policy)\n",
    "    determinstic_policy_eps = determinstic_policy_eps/np.sum(determinstic_policy_eps, axis = 0)\n",
    "\n",
    "    # print(determinstic_policy_eps)\n",
    "\n",
    "    Qisa = mae.Qisa(determinstic_policy)\n",
    "    # print(Qioa)\n",
    "    #for now checking for all possible states even the ones not visited\n",
    "    #need to remove discard degraded state\n",
    "\n",
    "\n",
    "    reverse_of_determinstic_policy = 1 - determinstic_policy\n",
    "\n",
    "    agents = 0\n",
    "    actions = 1\n",
    "    states = 2\n",
    "\n",
    "    # Qi = Qioa[:, :, 0]\n",
    "    # Qi_reverse = Qioa[:, :, 1]\n",
    "\n",
    "    \n",
    "    Qi = jnp.einsum(Qioa, [agents, states, actions],  determinstic_policy, [agents, states, actions], [agents, states])  #Here, we're calculating the average Q value for each agent - across the policy \n",
    "    Qi_reverse = jnp.einsum(Qioa, [agents, states, actions],  reverse_of_determinstic_policy, [agents, states, actions], [agents, states]) \n",
    "\n",
    "    print(\"Qioa \\n\", Qioa)\n",
    "    print(\"Qi \\n\" , Qi)\n",
    "    print(\"Qi_reverse \\n\", Qi_reverse)\n",
    "\n",
    "\n",
    "    tolerance  = 0\n",
    "    stability = np.all(Qi >= Qi_reverse - tolerance)\n",
    "\n",
    "    return stability\n",
    "\n",
    "\n",
    "E = 0.01\n",
    "policy = create_policy_from_strategy([0 + E] , [0 + E])\n",
    "\n",
    "check_policy_for_stability_Q_values(policy, 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17083f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_stability_all_determinstic_strategies(mode):\n",
    "\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "\n",
    "    index = 1\n",
    "    for i, p1_strategy in enumerate(list(strategy_set_p1.values())):\n",
    "        for j, p2_strategy in enumerate(list(strategy_set_p2.values())):\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            stability = check_policy_for_stability_Q_values(policy, mode)\n",
    "            \n",
    "            # print(stability)\n",
    "            if stability == True:\n",
    "                print(index, (list(strategy_set_p1.keys()))[i], (list(strategy_set_p2.keys()))[j])\n",
    "                index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022bcbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===  mode: none ===\n",
      "1 [0.01] [0.01]\n",
      "===  mode: ecological ===\n",
      "1 [0.99] [0.99]\n",
      "2 [0.01] [0.01]\n",
      "===  mode: social ===\n",
      "1 [0.01, 0.01, 0.01, 0.01] [0.01, 0.01, 0.01, 0.01]\n",
      "===  mode: complete ===\n",
      "1 [0.99, 0.99, 0.99, 0.99] [0.99, 0.99, 0.99, 0.99]\n",
      "2 [0.99, 0.01, 0.01, 0.99] [0.99, 0.01, 0.01, 0.99]\n",
      "3 [0.01, 0.01, 0.01, 0.99] [0.01, 0.01, 0.01, 0.99]\n",
      "4 [0.01, 0.01, 0.01, 0.01] [0.01, 0.01, 0.01, 0.01]\n"
     ]
    }
   ],
   "source": [
    "for mode in [ 'none', \"ecological\", 'social', 'complete']:\n",
    "    print(f\"===  mode: {mode} ===\")\n",
    "    check_for_stability_all_determinstic_strategies(mode)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f005314",
   "metadata": {},
   "source": [
    "\n",
    "##Q results\n",
    "\n",
    "===  mode: none ===\n",
    "1 [0.01] [0.01]\n",
    "===  mode: ecological ===\n",
    "1 [0.99] [0.99]\n",
    "2 [0.01] [0.01]\n",
    "\n",
    "===  mode: social ===\n",
    "1 [0.01, 0.01, 0.01, 0.01] [0.01, 0.01, 0.01, 0.01]\n",
    "===  mode: complete ===\n",
    "1 [0.99, 0.99, 0.99, 0.99] [0.99, 0.99, 0.99, 0.99]\n",
    "2 [0.99, 0.01, 0.01, 0.99] [0.99, 0.01, 0.01, 0.99]\n",
    "3 [0.01, 0.01, 0.01, 0.99] [0.01, 0.01, 0.01, 0.99]\n",
    "4 [0.01, 0.01, 0.01, 0.01] [0.01, 0.01, 0.01, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[50.   51.91]]\n",
      "\n",
      " [[50.   51.91]]]\n",
      "[[[1 0]]\n",
      "\n",
      " [[1 0]]]\n",
      "Qi [50. 50.]\n",
      "Qi_reverse [51.91 51.91]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = create_mae_ecopg_for_given_mode_POstratAC('none')\n",
    "E = 0.0\n",
    "policy = create_policy_from_strategy([1], [1])\n",
    "Qioa = mae.Qioa(policy)\n",
    "\n",
    "reverse_policy = 1 - policy\n",
    "\n",
    "print(Qioa)\n",
    "print(policy)\n",
    "Vio = mae.Vio(policy)\n",
    "# print(Qioa)\n",
    "\n",
    "obsdist = mae.obsdist(policy)\n",
    "\n",
    "# Qioa_2 =  np.random.randint(low = 0, high = 2, size = Qioa.shape)\n",
    "# print(Qioa_2, \"Qioa_2\")\n",
    "# obsdist_2 = np.random.randint(low = 0, high = 2, size = obsdist.shape)\n",
    "# print(obsdist_2, \"obsdist_2\")\n",
    "# print(obsdist)\n",
    "\n",
    "agents = 0\n",
    "actions = 1\n",
    "states = 2\n",
    "\n",
    "Qi = jnp.einsum(Qioa, [agents, states, actions],  policy, [agents, states, actions], [agents])  #Here, we're calculating the average Q value for each agent - across the policy and obsdist\n",
    "print(\"Qi\", Qi)\n",
    "\n",
    "Qi_reverse = jnp.einsum(Qioa, [agents, states, actions],  reverse_policy, [agents, states, actions], [agents])  #Here, we're calculating the average Q value for each agent - across the policy and obsdist\n",
    "print(\"Qi_reverse\", Qi_reverse)\n",
    "\n",
    "# avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "# print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "\n",
    "np.all(Qi < Qi_reverse)\n",
    "# print(Qioa[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985cfdc",
   "metadata": {},
   "source": [
    "#strategy vs actions\n",
    "\n",
    "WSLS - [1, 0, 0, 1] \n",
    "Q_ioa for each. \n",
    "\n",
    "0.999\n",
    "\n",
    "value functions -> policy (Q and SA)\n",
    "policy -> actor critic\n",
    "\n",
    "\n",
    "actor critic also uses Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a19e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Nash  3\n",
      "P1: [0.99 0.01 0.01 0.99]\n",
      "P2: [0.99 0.01 0.01 0.99]\n",
      "[[77.3666 76.5826 76.5825 77.3665]\n",
      " [77.3666 76.5826 76.5825 77.3665]] vis\n",
      "[0.9607 0.0099 0.0099 0.0195] state dist\n",
      "77.35103 77.35103 Vi\n",
      "(Array(77.351, dtype=float32), Array(77.351, dtype=float32))\n",
      "------\n",
      "P1: [0.99 0.01 0.01 0.01]\n",
      "P2: [0.99 0.01 0.01 0.01]\n",
      "[[27.0722  0.7635  0.7635  0.7635]\n",
      " [27.0722  0.7635  0.7635  0.7635]] vis\n",
      "[0.005  0.0099 0.0099 0.9752] state dist\n",
      "0.8950479 0.89504796 Vi\n",
      "(Array(0.895, dtype=float32), Array(0.895, dtype=float32))\n",
      "------\n",
      "P1: [0.01 0.01 0.01 0.01]\n",
      "P2: [0.01 0.01 0.01 0.01]\n",
      "[[0.503 0.503 0.503 0.503]\n",
      " [0.503 0.503 0.503 0.503]] vis\n",
      "[0.0001 0.0099 0.0099 0.9801] state dist\n",
      "0.5030022 0.5030023 Vi\n",
      "(Array(0.503, dtype=float32), Array(0.503, dtype=float32))\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_and_print_nash_equilibria_results_test(nash_equlibria_result, strategy_set_p1, strategy_set_p2, mode, mae):\n",
    "\n",
    "    number_of_nash_equilbria = len(nash_equlibria_result)\n",
    "    print(\"Total number of Nash \", number_of_nash_equilbria)\n",
    "\n",
    "    '''better readable format of the strategies'''\n",
    "    strategies_p1_array = np.array(list(strategy_set_p1.values())) #just the list of strategies\n",
    "    strategies_p2_array = np.array(list(strategy_set_p2.values()))\n",
    "\n",
    "    for eq in nash_equlibria_result:                       # each equilibrium profile\n",
    "        strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]    #converting mixed strategis to float\n",
    "\n",
    "        p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool) #boolean conversion - for strategy in pure nash probability =  1, for strategy not in pure nash, probability = 0\n",
    "        p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "        # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "        p1_nash_strategy = (strategies_p1_array[p1_active_strategy])[0]\n",
    "        p2_nash_strategy =  (strategies_p2_array[p2_active_strategy])[0]\n",
    "\n",
    "        \n",
    "        print('P1:' , p1_nash_strategy)   #will only print the strategy found in nash\n",
    "        print('P2:' , p2_nash_strategy)\n",
    "\n",
    "        policy_test = create_policy_from_strategy(p1_nash_strategy, p2_nash_strategy)\n",
    "\n",
    "        print(calculate_avg_value_given_policy_state_test_with_print(policy_test, mode, mae))\n",
    "\n",
    "        print('------')\n",
    "\n",
    "\n",
    "def calculate_avg_value_given_policy_state_test_with_print(policy, mode, mae):\n",
    "\n",
    "    Vis = mae.Vis(policy)\n",
    "    state_dist = mae.Ps(policy)\n",
    "\n",
    "    print(Vis, \"vis\")\n",
    "    print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "\n",
    "    print(avg_value_agent_1, avg_value_agent_2, \"Vi\")\n",
    "\n",
    "    return  avg_value_agent_1, avg_value_agent_2\n",
    "\n",
    "\n",
    "'''Memo1pd testing for understanding '''\n",
    "\n",
    "\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "\n",
    "\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "\n",
    "\n",
    "\n",
    "process_and_print_nash_equilibria_results_test(result.equilibria, strategy_set_p1, strategy_set_p2, mode, mae_socdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd252aca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "metagame_reward_matrix_obs_ecopg() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n\u001b[32m     13\u001b[39m mae_socdi = stratAC(env=env_memo1pd, learning_rates=\u001b[32m0.1\u001b[39m, discount_factors= \u001b[32m0.99\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m p1_reward_matrix_only_state, p2_reward_matrix_only_state = \u001b[43mmetagame_reward_matrix_obs_ecopg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy_set_p1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_set_p2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmae_socdi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m payoffs_row = p1_reward_matrix_only_state\n\u001b[32m     18\u001b[39m payoffs_col  = p2_reward_matrix_only_state\n",
      "\u001b[31mTypeError\u001b[39m: metagame_reward_matrix_obs_ecopg() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "'''Memo1pd testing for understanding '''\n",
    "\n",
    "\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "\n",
    "\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "\n",
    "\n",
    "\n",
    "process_and_print_nash_equilibria_results_test(result.equilibria, strategy_set_p1, strategy_set_p2, mode, mae_socdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db51f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1, 1, 1, 1] [1, 1, 1, 1]\n",
      "2 [1, 1, 1, 1] [1, 0, 1, 1]\n",
      "3 [1, 1, 1, 1] [0, 1, 1, 1]\n",
      "4 [1, 1, 1, 0] [1, 1, 1, 0]\n",
      "5 [1, 1, 1, 0] [1, 0, 1, 1]\n",
      "6 [1, 1, 1, 0] [1, 0, 1, 0]\n",
      "7 [1, 1, 1, 0] [0, 1, 1, 0]\n",
      "8 [1, 1, 1, 0] [0, 0, 1, 0]\n",
      "9 [1, 1, 1, 0] [0, 0, 0, 0]\n",
      "10 [1, 1, 0, 1] [1, 1, 1, 1]\n",
      "11 [1, 1, 0, 1] [1, 1, 1, 0]\n",
      "12 [1, 1, 0, 1] [1, 0, 1, 1]\n",
      "13 [1, 1, 0, 1] [1, 0, 1, 0]\n",
      "14 [1, 1, 0, 1] [0, 1, 1, 1]\n",
      "15 [1, 1, 0, 1] [0, 0, 1, 1]\n",
      "16 [1, 1, 0, 0] [1, 1, 1, 0]\n",
      "17 [1, 1, 0, 0] [1, 0, 1, 1]\n",
      "18 [1, 1, 0, 0] [0, 0, 1, 1]\n",
      "19 [1, 0, 1, 1] [0, 1, 1, 1]\n",
      "20 [1, 0, 1, 1] [0, 1, 0, 1]\n",
      "21 [1, 0, 1, 1] [0, 0, 1, 0]\n",
      "22 [1, 0, 1, 1] [0, 0, 0, 0]\n",
      "23 [1, 0, 1, 0] [0, 1, 0, 1]\n",
      "24 [1, 0, 1, 0] [0, 0, 1, 0]\n",
      "25 [1, 0, 1, 0] [0, 0, 0, 1]\n",
      "26 [1, 0, 0, 0] [0, 1, 0, 0]\n",
      "27 [0, 1, 1, 1] [1, 1, 1, 1]\n",
      "28 [0, 1, 1, 1] [1, 1, 0, 1]\n",
      "29 [0, 1, 1, 1] [1, 0, 1, 1]\n",
      "30 [0, 1, 1, 1] [0, 1, 1, 1]\n",
      "31 [0, 1, 1, 1] [0, 0, 1, 1]\n",
      "32 [0, 1, 1, 0] [1, 1, 1, 0]\n",
      "33 [0, 1, 1, 0] [0, 1, 1, 0]\n",
      "34 [0, 1, 1, 0] [0, 1, 0, 0]\n",
      "35 [0, 1, 0, 1] [1, 0, 1, 1]\n",
      "36 [0, 1, 0, 1] [1, 0, 1, 0]\n",
      "37 [0, 1, 0, 1] [0, 1, 1, 1]\n",
      "38 [0, 1, 0, 0] [1, 1, 1, 0]\n",
      "39 [0, 1, 0, 0] [1, 1, 0, 1]\n",
      "40 [0, 1, 0, 0] [1, 1, 0, 0]\n",
      "41 [0, 0, 1, 1] [1, 1, 0, 1]\n",
      "42 [0, 0, 1, 1] [1, 1, 0, 0]\n",
      "43 [0, 0, 1, 0] [1, 0, 0, 0]\n",
      "44 [0, 0, 1, 0] [0, 1, 1, 0]\n",
      "45 [0, 0, 1, 0] [0, 1, 0, 0]\n",
      "46 [0, 0, 0, 1] [1, 1, 0, 0]\n",
      "47 [0, 0, 0, 0] [1, 1, 1, 0]\n",
      "48 [0, 0, 0, 0] [1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "\n",
    "def check_policy_for_stability_Q_values_state(policy):\n",
    "    agents = 0\n",
    "    actions = 1\n",
    "    states = 2\n",
    "\n",
    "    Qisa = mae_socdi.Qisa(policy)\n",
    "\n",
    "    reverse_policy = 1 - policy\n",
    "\n",
    "\n",
    "\n",
    "    Qi = jnp.einsum(Qisa, [agents, states, actions],  policy, [agents, states, actions], [agents])  #Here, we're calculating the average Q value for each agent - across the policy and obsdist\n",
    "\n",
    "    Qi_reverse = jnp.einsum(Qisa, [agents, states, actions],  reverse_policy, [agents, states, actions], [agents])  #Here, we're calculating the average Q value for each agent - across the policy and obsdist\n",
    "\n",
    "    # avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "    # print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "\n",
    "    stability =  np.all(Qi < Qi_reverse)\n",
    "    return stability\n",
    "\n",
    "\n",
    "\n",
    "def check_policy_for_stability_Q_values_state_all():\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players('social')\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "    strategy_set_p2 = list(strategy_set_p2.values())\n",
    "\n",
    "    index = 1\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2):\n",
    "\n",
    "            policy  = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            stability = check_policy_for_stability_Q_values_state(policy)\n",
    "            # print(stability)\n",
    "            if stability == True:\n",
    "                print(index, strategy_set_p1[i], strategy_set_p2[j])\n",
    "                index = index + 1\n",
    "\n",
    "check_policy_for_stability_Q_values_state_all()\n",
    "# print(Qioa[0,:,:])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c507c",
   "metadata": {},
   "source": [
    "Reward prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a28c4a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1 0]\n",
      "   [0 1]\n",
      "   [0 1]\n",
      "   [1 0]]\n",
      "\n",
      "  [[1 0]\n",
      "   [0 1]\n",
      "   [0 1]\n",
      "   [1 0]]]\n",
      "\n",
      "\n",
      " [[[1 0]\n",
      "   [0 1]\n",
      "   [0 1]\n",
      "   [0 1]]\n",
      "\n",
      "  [[1 0]\n",
      "   [0 1]\n",
      "   [0 1]\n",
      "   [0 1]]]\n",
      "\n",
      "\n",
      " [[[0 1]\n",
      "   [0 1]\n",
      "   [0 1]\n",
      "   [0 1]]\n",
      "\n",
      "  [[0 1]\n",
      "   [0 1]\n",
      "   [0 1]\n",
      "   [0 1]]]]\n"
     ]
    }
   ],
   "source": [
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "\n",
    "def RPE_stability_check(policy):\n",
    "\n",
    "    prob_of_coop = policy[:, :, 0]\n",
    "    # print(prob_of_coop)\n",
    "\n",
    "    reward_prediction_error = mae_socdi.RPEisa(policy)\n",
    "    # print(reward_prediction_error)\n",
    "    argmin_Q = np.argmin(reward_prediction_error, axis = 2) #if erro i s -.5 theb the poliviy id \n",
    "    # print(argmin_Q)\n",
    "\n",
    "    stability = np.all(prob_of_coop == argmin_Q)\n",
    "    \n",
    "    return stability\n",
    "\n",
    "\n",
    "\n",
    "def check_policy_for_stability_RPE_all():\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players('social')\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    \n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    results = list(map(RPE_stability_check, policies))\n",
    "    # print(results)\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(policies[results])\n",
    "\n",
    "\n",
    "check_policy_for_stability_RPE_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551a154",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RPE_stability_check() missing 1 required positional argument: 'mae'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# print(\"==\", mode, \"===\")\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(policies[results])\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mcheck_policy_for_stability_RPE_all_socdi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mcheck_policy_for_stability_RPE_all_socdi\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,\u001b[32m2\u001b[39m)\n\u001b[32m     23\u001b[39m policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) \u001b[38;5;28;01mfor\u001b[39;00m p1_strategy, p2_strategy \u001b[38;5;129;01min\u001b[39;00m strategy_combinations])\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRPE_stability_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# print(\"==\", mode, \"===\")\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: RPE_stability_check() missing 1 required positional argument: 'mae'"
     ]
    }
   ],
   "source": [
    "def RPE_stability_check(policy, mae):\n",
    "\n",
    "    prob_of_coop = policy[:, :, 0]\n",
    "    # print(prob_of_coop)\n",
    "\n",
    "    reward_prediction_error = mae.RPEisa(policy)\n",
    "    # print(reward_prediction_error)\n",
    "    argmin_Q = np.argmin(reward_prediction_error, axis = 2) #if erro i s -.5 theb the poliviy id \n",
    "    # print(argmin_Q)\n",
    "\n",
    "    stability = np.all(prob_of_coop == argmin_Q)\n",
    "    \n",
    "    return stability\n",
    "\n",
    "\n",
    "\n",
    "def check_policy_for_stability_RPE_all_socdi():\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players('social')\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    mae = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "\n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    results = list(map(RPE_stability_check, policies))\n",
    "    print(results)\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(policies[results])\n",
    "\n",
    "\n",
    "check_policy_for_stability_RPE_all_socdi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e84905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def RPE_stability_check(policy, mae, mode):\n",
    "\n",
    "    prob_of_coop = policy[:, :, 0]  #Take only probability of cooperation\n",
    "    # print(prob_of_coop)\n",
    "    \n",
    "    reward_prediction_error = mae.RPEioa(policy)   \n",
    "    # print('reward_prediction_error: ', reward_prediction_error)\n",
    "    argmin_Q = np.argmin(reward_prediction_error, axis = 2)  # argmin 0 implies RPE is for reducing cooperation, 1 implies RPE is for increasing cooperation\n",
    "    obsdist = mae.obsdist(policy)\n",
    "    # print(\"obsdist \\n\", obsdist)\n",
    "\n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        prob_of_coop = prob_of_coop[:,1::2]\n",
    "        argmin_Q = argmin_Q[:, 1::2]\n",
    "\n",
    "    # print(\"prob_of_coop \\n\", prob_of_coop)\n",
    "    # print(\"argin_Q \\n\", argmin_Q)\n",
    "    print(\"---\")\n",
    "    # print(reward_prediction_error)\n",
    "    stability = np.all(prob_of_coop == argmin_Q)  #if for example, for a particular state you get a zero (RPE reduces for cooperation, ) but your policy is 0. So vector is in the direction of what you're doing.\n",
    "                                                 #If for a particular state argmin index is 1, then it means you should reduce defection, (thus increase cooperation). if your policy is 0 (no cooperation), then the vector is in the oppoosite of what youre doing\n",
    "    return stability\n",
    "\n",
    "# mae_mode = create_mae_ecopg_for_given_mode_POstratAC('social')\n",
    "\n",
    "# policy = create_policy_from_strategy([1,1,1,1], [1,1,1,1])\n",
    "# RPE_stability_check(policy, mae_mode)\n",
    "\n",
    "def check_policy_for_stability_RPE_all(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players(mode)\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    mae = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    # print(policies[-1])\n",
    "    RPE_stability_check_mae = partial(RPE_stability_check, mae = mae, mode = mode)\n",
    "    results = list(map(RPE_stability_check_mae, policies))\n",
    "    # print(results)\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(policies[results])\n",
    "\n",
    "\n",
    "check_policy_for_stability_RPE_all('social')\n",
    "\n",
    "\n",
    "# policy = create_policy_from_strategy([0,0,0, 0], [0,0,0,0])\n",
    "# RPE_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "174d8d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.      0.    ]\n",
      "  [-0.0096  0.    ]\n",
      "  [-0.0097  0.    ]\n",
      "  [ 0.2827 -0.0003]]\n",
      "\n",
      " [[ 0.      0.    ]\n",
      "  [-0.0096  0.    ]\n",
      "  [-0.0097  0.    ]\n",
      "  [ 0.2827 -0.0003]]]\n",
      "[[[[0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.001 0.999]]\n",
      "\n",
      "  [[0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.001 0.999]]]\n",
      "\n",
      "\n",
      " [[[0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.001 0.999]]\n",
      "\n",
      "  [[0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.    1.   ]\n",
      "   [0.001 0.999]]]]\n",
      "TDe: \n",
      " [[[ 0.      0.    ]\n",
      "  [-0.0096  0.    ]\n",
      "  [-0.0097  0.    ]\n",
      "  [ 0.2827 -0.0003]]\n",
      "\n",
      " [[ 0.      0.    ]\n",
      "  [-0.0096  0.    ]\n",
      "  [-0.0097  0.    ]\n",
      "  [ 0.2827 -0.0003]]]\n",
      "XexpaTDe: \n",
      " [[[0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.001 0.999]]\n",
      "\n",
      " [[0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.001 0.999]]]\n",
      "[[[0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.001 0.999]]\n",
      "\n",
      " [[0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.    1.   ]\n",
      "  [0.001 0.999]]]\n"
     ]
    }
   ],
   "source": [
    "mae_social_info = create_mae_ecopg_for_given_mode_POstratAC('social')\n",
    "policy = create_policy_from_strategy([0,0,0, 0.001], [0,0,0,0.001])\n",
    "\n",
    "\n",
    "result = run_simulation_for_initial_condition_with_traj(\n",
    "                mae = mae_social_info, \n",
    "                initial_condition = policy,\n",
    "                make_degraded_state_cooperation_probablity_zero_at_end= False,\n",
    "                make_degraded_state_obsdist_zero_at_end= False\n",
    "            )  \n",
    "\n",
    "\n",
    "print(mae_social_info.TDerror(policy))\n",
    "print(result['xtraj'])\n",
    "\n",
    "alpha = mae_social_info.alpha\n",
    "TDe = mae_social_info.TDerror(policy)\n",
    "print('TDe: \\n', TDe)\n",
    "n = jnp.newaxis\n",
    "XexpaTDe = policy * jnp.exp(alpha[:,n,n] * TDe)\n",
    "print('XexpaTDe: \\n', XexpaTDe)\n",
    "X_next = XexpaTDe / XexpaTDe.sum(-1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "print(mae_social_info.step(policy)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dadb54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "### testing this for socdi\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "\n",
    "\n",
    "def next_step_stability_check(policy, mae, mode):\n",
    "\n",
    "    # prob_of_coop = policy[:, :, 0]  #Take only probability of cooperation\n",
    "    # print(prob_of_coop)\n",
    "    \n",
    "    next_step_policy = mae.step(policy)[0] \n",
    "    # print('policy: ', policy)\n",
    "    # print('next_step_policy: ', next_step_policy)\n",
    "    # print('reward_prediction_error: ', reward_prediction_error)\n",
    "\n",
    "    # if mode == 'complete' or mode == 'ecological':\n",
    "    #     prob_of_coop = prob_of_coop[:,1::2]\n",
    "    #     argmin_Q = argmin_Q[:, 1::2]\n",
    "\n",
    "    stability = False\n",
    "    if np.allclose(next_step_policy, policy, rtol = 0, atol = 0):\n",
    "        stability = True  #if for example, for a particular state you get a zero (RPE reduces for cooperation, ) but your policy is 0. So vector is in the direction of what you're doing.\n",
    "                        #If for a particular state argmin index is 1, then it means you should reduce defection, (thus increase cooperation). if your policy is 0 (no cooperation), then the vector is in the oppoosite of what youre doing\n",
    "    return stability\n",
    "\n",
    "# mae_mode = create_mae_ecopg_for_given_mode_POstratAC('social')\n",
    "# e = 1e-5\n",
    "# policy = create_policy_from_strategy([0,0 +e,1,0], [0,0,0,0])\n",
    "# print(next_step_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social'))\n",
    "\n",
    "def check_policy_for_stability_next_step_all(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode,0)\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    mae = mae_socdi\n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    # print(policies[-1])\n",
    "    next_step_stability_check_mae = partial(next_step_stability_check, mae = mae, mode = mode)\n",
    "    results = list(map(next_step_stability_check_mae, policies))\n",
    "    stable_policies = policies[results]\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(len(stable_policies))\n",
    "    print(stable_policies)\n",
    "\n",
    "\n",
    "# check_policy_for_stability_RPE_all('social')\n",
    "\n",
    "\n",
    "# policy = create_policy_from_strategy([0,0,0,0], [0,0,0,0])\n",
    "# next_step_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social')\n",
    "\n",
    "check_policy_for_stability_next_step_all('social')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c160d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy \n",
      " [[[0.      1.     ]\n",
      "  [0.00001 0.99999]\n",
      "  [1.      0.     ]\n",
      "  [0.      1.     ]]\n",
      "\n",
      " [[0.      1.     ]\n",
      "  [0.      1.     ]\n",
      "  [0.      1.     ]\n",
      "  [0.      1.     ]]]\n",
      "next_step_policy \n",
      " [[[0.      1.     ]\n",
      "  [0.00001 0.99999]\n",
      "  [1.      0.     ]\n",
      "  [0.      1.     ]]\n",
      "\n",
      " [[0.      1.     ]\n",
      "  [0.      1.     ]\n",
      "  [0.      1.     ]\n",
      "  [0.      1.     ]]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def next_step_stability_check(policy, mae, mode):\n",
    "\n",
    "    # prob_of_coop = policy[:, :, 0]  #Take only probability of cooperation\n",
    "    # print(prob_of_coop)\n",
    "    \n",
    "    next_step_policy = mae.step(policy)[0] \n",
    "    print('policy \\n', policy)\n",
    "    print('next_step_policy \\n', next_step_policy)\n",
    "    # print('reward_prediction_error: ', reward_prediction_error)\n",
    "\n",
    "    # if mode == 'complete' or mode == 'ecological':\n",
    "    #     prob_of_coop = prob_of_coop[:,1::2]\n",
    "    #     argmin_Q = argmin_Q[:, 1::2]\n",
    "\n",
    "    stability = False\n",
    "    if np.allclose(next_step_policy, policy, rtol = 1e-5, atol = 1e-8):\n",
    "        stability = True  #if for example, for a particular state you get a zero (RPE reduces for cooperation, ) but your policy is 0. So vector is in the direction of what you're doing.\n",
    "                        #If for a particular state argmin index is 1, then it means you should reduce defection, (thus increase cooperation). if your policy is 0 (no cooperation), then the vector is in the oppoosite of what youre doing\n",
    "    return stability\n",
    "\n",
    "# mae_mode = create_mae_ecopg_for_given_mode_POstratAC('social')\n",
    "e = 1e-5\n",
    "policy = create_policy_from_strategy([0,0 +e,1,0], [0,0,0,0])\n",
    "print(next_step_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social'))\n",
    "\n",
    "def check_policy_for_stability_next_step_all(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode, 1e-5)\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    mae = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    # print(policies[-1])\n",
    "    next_step_stability_check_mae = partial(next_step_stability_check, mae = mae, mode = mode)\n",
    "    results = list(map(next_step_stability_check_mae, policies))\n",
    "    stable_policies = policies[results]\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(len(stable_policies))\n",
    "    print(stable_policies)\n",
    "\n",
    "\n",
    "# check_policy_for_stability_RPE_all('social')\n",
    "\n",
    "\n",
    "# policy = create_policy_from_strategy([0,0,0,0], [0,0,0,0])\n",
    "# next_step_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social')\n",
    "\n",
    "\n",
    "# check_policy_for_stability_next_step_all('social')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step_stability_check(policy, mae, mode):\n",
    "\n",
    "    # prob_of_coop = policy[:, :, 0]  #Take only probability of cooperation\n",
    "    # print(prob_of_coop)\n",
    "    \n",
    "    next_step_policy = mae.step(policy)[0] \n",
    "    print('policy: ', policy)\n",
    "    print('next_step_policy: ', next_step_policy)\n",
    "    # print('reward_prediction_error: ', reward_prediction_error)\n",
    "\n",
    "    # if mode == 'complete' or mode == 'ecological':\n",
    "    #     prob_of_coop = prob_of_coop[:,1::2]\n",
    "    #     argmin_Q = argmin_Q[:, 1::2]\n",
    "\n",
    "    stability = False\n",
    "    if np.allclose(next_step_policy, policy, rtol = 1e-5, atol = 1e-8):\n",
    "        stability = True  #if for example, for a particular state you get a zero (RPE reduces for cooperation, ) but your policy is 0. So vector is in the direction of what you're doing.\n",
    "                        #If for a particular state argmin index is 1, then it means you should reduce defection, (thus increase cooperation). if your policy is 0 (no cooperation), then the vector is in the oppoosite of what youre doing\n",
    "    return stability\n",
    "\n",
    "# mae_mode = create_mae_ecopg_for_given_mode_POstratAC('social')\n",
    "e = 1e-5\n",
    "policy = create_policy_from_strategy([0,0 +e,1,0], [0,0,0,0])\n",
    "print(next_step_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social'))\n",
    "\n",
    "def check_policy_for_stability_next_step_all(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode, 1e-5)\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    mae = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies =  np.array([create_policy_from_strategy(p1_strategy, p2_strategy) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    # print(policies[-1])\n",
    "    next_step_stability_check_mae = partial(next_step_stability_check, mae = mae, mode = mode)\n",
    "    results = list(map(next_step_stability_check_mae, policies))\n",
    "    stable_policies = policies[results]\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(len(stable_policies))\n",
    "    print(stable_policies)\n",
    "\n",
    "\n",
    "# check_policy_for_stability_RPE_all('social')\n",
    "\n",
    "\n",
    "# policy = create_policy_from_strategy([0,0,0,0], [0,0,0,0])\n",
    "# next_step_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social')\n",
    "\n",
    "\n",
    "# check_policy_for_stability_next_step_all('social')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "643325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e6ebd847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "def next_step_stability_check_norm(noisy_policy, determinstic_policy, mae, mode):\n",
    "\n",
    "    # prob_of_coop = policy[:, :, 0]  #Take only probability of cooperation\n",
    "    # print(prob_of_coop)\n",
    "    # print(noisy_policy)\n",
    "    next_step_policy = mae.step(noisy_policy)[0] \n",
    "    # print('policy: ', policy)\n",
    "    # print('next_step_policy: ', next_step_policy)\n",
    "    # print('reward_prediction_error: ', reward_prediction_error)\n",
    "\n",
    "    # if mode == 'complete' or mode == 'ecological':\n",
    "    #     prob_of_coop = prob_of_coop[:,1::2]\n",
    "    #     argmin_Q = argmin_Q[:, 1::2]\n",
    "\n",
    "    stability = False\n",
    "    if np.linalg.norm(next_step_policy - determinstic_policy) <  np.linalg.norm(noisy_policy - determinstic_policy):\n",
    "        stability = True\n",
    "        \n",
    "                          #if for example, for a particular state you get a zero (RPE reduces for cooperation, ) but your policy is 0. So vector is in the direction of what you're doing.\n",
    "                        #If for a particular state argmin index is 1, then it means you should reduce defection, (thus increase cooperation). if your policy is 0 (no cooperation), then the vector is in the oppoosite of what youre doing\n",
    "    return stability\n",
    "\n",
    "# mae_mode = create_mae_ecopg_for_given_mode_POstratAC('social')\n",
    "# epsilon = 1e-4\n",
    "# noisy_policy, determinstic_policy = create_policy_with_noise_from_deterministic_strategy([0 ,0,0, 0], [0,0,0,0], epsilon)\n",
    "# print(next_step_stability_check(noisy_policy, determinstic_policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social'))\n",
    "\n",
    "def check_policy_for_stability_next_step_all(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players(mode)\n",
    "    strategy_set_p1 = list(strategy_set_p1.values())\n",
    "\n",
    "    mae = mae_socdi\n",
    "    strategy_combinations = itertools.combinations_with_replacement(strategy_set_p1,2)\n",
    "    policies_list =  np.array([create_policy_with_noise_from_deterministic_strategy(p1_strategy, p2_strategy, 1e-3) for p1_strategy, p2_strategy in strategy_combinations])\n",
    "    stability_list = []\n",
    "    for policies in policies_list:\n",
    "        noisy_policy, determinstic_policy = policies\n",
    "        stability = next_step_stability_check_norm(noisy_policy, determinstic_policy, mae, mode)\n",
    "        stability_list.append(stability)\n",
    "\n",
    "\n",
    "    # next_step_stability_check_mae = partial(next_step_stability_check_norm, mae = mae, mode = mode)\n",
    "    # results = list(map(next_step_stability_check_mae, policies))\n",
    "    determinstic_policy_list = np.array([determinstic_policy for (noisy_policy, determinstic_policy) in policies_list])\n",
    "    # print(stability_list)\n",
    "    stable_policies = determinstic_policy_list[stability_list]\n",
    "    # print(\"==\", mode, \"===\")\n",
    "    print(len(stable_policies))\n",
    "    # print(stable_policies)\n",
    "\n",
    "\n",
    "# check_policy_for_stability_RPE_all('social')\n",
    "\n",
    "\n",
    "# policy = create_policy_from_strategy([0,0,0,0], [0,0,0,0])\n",
    "# next_step_stability_check(policy, create_mae_ecopg_for_given_mode_POstratAC('social'), 'social')\n",
    "\n",
    "\n",
    "check_policy_for_stability_next_step_all('social')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
