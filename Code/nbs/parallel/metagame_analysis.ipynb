{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095b105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from information_conditions import Information_Conditions\n",
    "from base_ecopg import *\n",
    "from helper_functions import *\n",
    "from simulation_and_results_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38583a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b957bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_degraded_state_policies_both_state_and_action(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "\n",
    "        for i in [0, 2, 4, 6]:\n",
    "            x = np.random.rand()\n",
    "            strategy_propserous_and_degraded_state.insert(i, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n",
    "\n",
    "def add_degraded_state_policies_only_state(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "        x = np.random.rand()\n",
    "        strategy_propserous_and_degraded_state.insert(0, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52af6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_from_strategy(agent_1_strategy, agent_2_strategy):\n",
    "\n",
    "\n",
    "    agent_1_strategy = [[x, 1-x] for x in agent_1_strategy]\n",
    "    agent_2_strategy = [[x, 1-x] for x in agent_2_strategy]\n",
    "\n",
    "    policy = np.array([agent_1_strategy, agent_2_strategy])\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58d1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy_obs(policy, mode):\n",
    "\n",
    "    mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    # mae_ecopg.has_last_statdist = False\n",
    "    # mae_ecopg._last_statedist = None\n",
    "\n",
    "    Vio = mae_ecopg.Vio(policy)\n",
    "    obsdist = mae_ecopg.obsdist(policy)\n",
    "\n",
    "    print(Vio, \"Vio\")\n",
    "    print(obsdist, \"obsdist\")\n",
    "\n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vio = Vio[:,1::2]\n",
    "        obsdist = obsdist[:,1::2]\n",
    "        obsdist = obsdist/obsdist.sum(axis = 1)\n",
    "\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "    print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "    return avg_value_agent_1, avg_value_agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52a781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy_state_mae_ecopg(policy, mode):\n",
    "\n",
    "    mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    Vis = mae_ecopg.Vis(policy)\n",
    "    mae_ecopg.has_last_statdist = False\n",
    "    mae_ecopg._last_statedist = None\n",
    "    state_dist = mae_ecopg._numpyPs(policy)\n",
    "\n",
    "    print(Vis, \"vis\")\n",
    "    print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "    print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "    return avg_value_agent_1, avg_value_agent_2\n",
    "\n",
    "\n",
    "\n",
    "def calculate_avg_value_given_policy_state_test(policy, mode, mae):\n",
    "\n",
    "    Vis = mae.Vis(policy)\n",
    "    state_dist = mae.Ps(policy)\n",
    "\n",
    "    # print(Vis, \"vis\")\n",
    "    # print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "\n",
    "\n",
    "    return  avg_value_agent_1, avg_value_agent_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6629c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode):\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "    \n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_obs(policy, mode)\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_mae_ecopg(policy, mode)\n",
    "\n",
    "            # print(value_p1, value_p2 ,\"value p1, p2\")\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metagame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode, mae):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_test(policy, mode, mae)\n",
    "\n",
    "            # print(value_p1, value_p2 ,\"value p1, p2\")\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db3dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players(mode):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    E = 0.001\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 - E , 0 + E], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1 - E , 0 + E], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3200d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_matrix_to_excel(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    '''p1 rewards pg 1. p2 rewards pg 2. rows -> p1 strategies, cols -> p2 strategies in both cases '''\n",
    "    p1_reward_matrix = pd.DataFrame(p1_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "    p2_reward_matrix = pd.DataFrame(p2_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "\n",
    "    with pd.ExcelWriter('reward_matrix_only_action_2.xlsx') as excel_file:\n",
    "        p1_reward_matrix.to_excel(excel_file, sheet_name='p1', index=True, header=True)\n",
    "        p2_reward_matrix.to_excel(excel_file, sheet_name='p2', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7dd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pure_strategy_nash_equilibria(payoffs_row, payoffs_col):\n",
    "    game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "    result = pygambit.nash.enumpure_solve(game)\n",
    "    return result.equilibria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb89d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nstrategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\\nstrategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\\n\\nprint(strategy_set_p1_only_action_array)\\n\\nfor eq in result.equilibria:                       # each equilibrium profile\\n    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\\n\\n    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\\n    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\\n    # float_profile = np.array() # iterate over (strategy, prob\\n\\n    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\\n    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\\n\\n    print('------')\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_and_print_nash_equilibria_results(nash_equlibria_result, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    number_of_nash_equilbria = len(nash_equlibria_result)\n",
    "    print(\"Total number of Nash \", number_of_nash_equilbria)\n",
    "\n",
    "    '''better readable format of the strategies'''\n",
    "    strategies_p1_array = np.array(list(strategy_set_p1.keys())) #just the list of strategies\n",
    "    strategies_p2_array = np.array(list(strategy_set_p2.keys()))\n",
    "\n",
    "    for eq in nash_equlibria_result:                       # each equilibrium profile\n",
    "        strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]    #converting mixed strategis to float\n",
    "\n",
    "        p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool) #boolean conversion - for strategy in pure nash probability =  1, for strategy not in pure nash, probability = 0\n",
    "        p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "        # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "        print('P1:' , strategies_p1_array[p1_active_strategy])   #will only print the strategy found in nash\n",
    "        print('P2:' , strategies_p2_array[p2_active_strategy])\n",
    "\n",
    "        print('------')\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "strategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\n",
    "strategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "print(strategy_set_p1_only_action_array)\n",
    "\n",
    "for eq in result.equilibria:                       # each equilibrium profile\n",
    "    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\n",
    "\n",
    "    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\n",
    "    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "    # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\n",
    "    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\n",
    "\n",
    "    print('------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924aefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pure_nash_equilibria_all_deterministic_strategies_mode(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players(mode)\n",
    "    p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "    # p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "\n",
    "    print(\"p1: \\n\", p1_reward_matrix, \"\\n p2: \\n\", p2_reward_matrix)\n",
    "    \n",
    "    nash_equlibrium_result = calculate_pure_strategy_nash_equilibria(p1_reward_matrix, p2_reward_matrix)\n",
    "    process_and_print_nash_equilibria_results(nash_equlibrium_result, strategy_set_p1, strategy_set_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d452f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c,c,g|',\n",
       " 'c,c,p|',\n",
       " 'c,d,g|',\n",
       " 'c,d,p|',\n",
       " 'd,c,g|',\n",
       " 'd,c,p|',\n",
       " 'd,d,g|',\n",
       " 'd,d,p|']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = create_mae_ecopg_for_given_mode_POstratAC('none')\n",
    "mae.env.Sset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1166f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-298.2957   49.6027 -298.2957   49.6027 -298.2957   49.6027 -298.2957   49.6027]\n",
      " [-298.2957   49.6027 -298.2957   49.6027 -298.2957   49.6027 -298.2957   49.6027]] vis\n",
      "[0.1661 0.8319 0.0002 0.0008 0.0002 0.0008 0.     0.    ] state dist\n",
      "-8.317024 -8.317024 value of agents\n",
      "[[-299.3505 -166.7343 -299.3505 -166.7343 -299.3505 -166.7343 -299.3505 -166.7343]\n",
      " [-298.5409   -0.6926 -298.5409   -0.6926 -298.5409   -0.6926 -298.5409   -0.6926]] vis\n",
      "[0.001  0.     0.9881 0.0099 0.     0.     0.001  0.    ] state dist\n",
      "-298.03732 -295.59158 value of agents\n",
      "[[-298.5409   -0.6926 -298.5409   -0.6926 -298.5409   -0.6926 -298.5409   -0.6926]\n",
      " [-299.3506 -166.7343 -299.3505 -166.7343 -299.3506 -166.7343 -299.3505 -166.7343]] vis\n",
      "[0.001  0.     0.     0.     0.9881 0.0099 0.001  0.    ] state dist\n",
      "-295.5916 -298.03738 value of agents\n",
      "[[-299.274  -151.0548 -299.274  -151.0548 -299.274  -151.0548 -299.274  -151.0548]\n",
      " [-299.274  -151.0548 -299.274  -151.0548 -299.274  -151.0548 -299.274  -151.0548]] vis\n",
      "[0.    0.    0.001 0.    0.001 0.    0.993 0.005] state dist\n",
      "-298.53577 -298.53577 value of agents\n",
      "p1: \n",
      " [[  -8.317  -298.0373]\n",
      " [-295.5916 -298.5358]] \n",
      " p2: \n",
      " [[  -8.317  -295.5916]\n",
      " [-298.0374 -298.5358]]\n",
      "Total number of Nash  1\n",
      "P1: ['[0.999]']\n",
      "P2: ['[0.999]']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "get_pure_nash_equilibria_all_deterministic_strategies_mode('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7149b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0]]\n",
      "\n",
      " [[1 0]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[50.  , 51.91]],\n",
       "\n",
       "       [[50.  , 51.91]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = create_mae_ecopg_for_given_mode_POstratAC('none')\n",
    "policy = create_policy_from_strategy([1], [1])\n",
    "print(policy)\n",
    "mae.Qioa(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985cfdc",
   "metadata": {},
   "source": [
    "#strategy vs actions\n",
    "\n",
    "WSLS - [1, 0, 0, 1] \n",
    "Q_ioa for each. \n",
    "\n",
    "0.999\n",
    "\n",
    "value functions -> policy (Q and SA)\n",
    "policy -> actor critic\n",
    "\n",
    "\n",
    "actor critic also uses Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7537f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'socdi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyCRLD\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mEnvironments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mHistoryEmbedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HistoryEmbedded\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m memo1pd = HistoryEmbedded(\u001b[43msocdi\u001b[49m, h=(\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'socdi' is not defined"
     ]
    }
   ],
   "source": [
    "from pyCRLD.Environments.HistoryEmbedding import HistoryEmbedded\n",
    "memo1pd = HistoryEmbedded(socdi, h=(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a795fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[0.9999, 0.9999, 0.9999, 0.9999]': [0.9999, 0.9999, 0.9999, 0.9999], '[0.9999, 0.9999, 0.9999, 0.0001]': [0.9999, 0.9999, 0.9999, 0.0001], '[0.9999, 0.9999, 0.0001, 0.9999]': [0.9999, 0.9999, 0.0001, 0.9999], '[0.9999, 0.9999, 0.0001, 0.0001]': [0.9999, 0.9999, 0.0001, 0.0001], '[0.9999, 0.0001, 0.9999, 0.9999]': [0.9999, 0.0001, 0.9999, 0.9999], '[0.9999, 0.0001, 0.9999, 0.0001]': [0.9999, 0.0001, 0.9999, 0.0001], '[0.9999, 0.0001, 0.0001, 0.9999]': [0.9999, 0.0001, 0.0001, 0.9999], '[0.9999, 0.0001, 0.0001, 0.0001]': [0.9999, 0.0001, 0.0001, 0.0001], '[0.0001, 0.9999, 0.9999, 0.9999]': [0.0001, 0.9999, 0.9999, 0.9999], '[0.0001, 0.9999, 0.9999, 0.0001]': [0.0001, 0.9999, 0.9999, 0.0001], '[0.0001, 0.9999, 0.0001, 0.9999]': [0.0001, 0.9999, 0.0001, 0.9999], '[0.0001, 0.9999, 0.0001, 0.0001]': [0.0001, 0.9999, 0.0001, 0.0001], '[0.0001, 0.0001, 0.9999, 0.9999]': [0.0001, 0.0001, 0.9999, 0.9999], '[0.0001, 0.0001, 0.9999, 0.0001]': [0.0001, 0.0001, 0.9999, 0.0001], '[0.0001, 0.0001, 0.0001, 0.9999]': [0.0001, 0.0001, 0.0001, 0.9999], '[0.0001, 0.0001, 0.0001, 0.0001]': [0.0001, 0.0001, 0.0001, 0.0001]}\n",
      "[[ 0.7999  0.7999  0.7998  0.7998  0.3664  0.1497  0.1502 -0.0665  0.1501  0.15    0.15    0.15   -0.4997 -0.4998 -0.4997 -0.4998]\n",
      " [ 0.7999  0.7999  0.7998  0.7997  0.3666  0.2339 -0.0662 -0.0497  0.1501  0.1201  0.15    0.12   -0.4995 -0.3331 -0.4997 -0.3331]\n",
      " [ 0.8665  0.8666  0.7998  0.7997  0.5248  0.1833  0.3668 -0.0666  0.3201  0.32    0.1501  0.15   -0.4997 -0.4998 -0.4997 -0.4998]\n",
      " [ 0.8999  0.7333  0.6499  0.3996  0.683   0.3248  0.3249 -0.033   0.5748  0.3251  0.325   0.075   0.25   -0.0828 -0.0004 -0.25  ]\n",
      " [ 0.7999  0.7999  0.5248  0.4331  0.7998  0.2752  0.7996 -0.0392  0.4333  0.4333  0.25    0.25    0.1001 -0.2498  0.1    -0.2499]\n",
      " [ 0.7999  0.7998  0.4332  0.3248  0.7998  0.3996  0.3252  0.      0.4333  0.325   0.25    0.1667  0.325   0.0001  0.1667  0.    ]\n",
      " [ 0.8999  0.9331  0.7998  0.325   0.8665  0.3251  0.7997 -0.0395  0.9996  0.9996  0.325   0.1667  0.325   0.0001  0.1001 -0.2499]\n",
      " [ 0.9332  0.6997  0.5601  0.0001  0.9331  0.4665  0.5599  0.0002  0.9997  0.6665  0.4999  0.0001  0.9996  0.3332  0.4999  0.0001]\n",
      " [ 0.8999  0.8998  0.4333  0.4333  0.6201  0.1994 -0.4994 -0.4997  0.4     0.1001  0.3999  0.1     0.1002 -0.4998 -0.0498 -0.4998]\n",
      " [ 0.8999  0.7202  0.4333  0.3251  0.6199  0.3247 -0.4995 -0.3331  0.5999  0.0002  0.325   0.0002  0.3249 -0.1665 -0.4995 -0.2498]\n",
      " [ 0.9999  0.9997  0.5999  0.325   0.9999  0.2499  0.325  -0.4996  0.6     0.3253  0.4     0.1     0.3249 -0.4987  0.1001 -0.4998]\n",
      " [ 0.9999  0.6665  0.5     0.0002  0.9999  0.4168  0.3001 -0.1666  0.9998  0.3335  0.5     0.0001  0.9996  0.125   0.3002 -0.1666]\n",
      " [ 0.8999  0.8998  0.2502  0.25    0.8998  0.3251  0.325  -0.2498  0.4001  0.325   0.325   0.2499  0.4    -0.2498  0.3998 -0.2499]\n",
      " [ 0.8999  0.72    0.2501  0.1667  0.8998  0.4502  0.1667  0.      0.6     0.0003  0.2501  0.1251  0.5999  0.0001  0.1667  0.    ]\n",
      " [ 0.9999  0.9997  0.5999  0.1667  0.9999  0.3749  0.5999 -0.2498  0.6997  0.9995  0.4     0.1667  0.5999  0.0004  0.3999 -0.2499]\n",
      " [ 0.9999  0.6665  0.5     0.0001  0.9999  0.5002  0.5     0.0001  0.9998  0.5001  0.5     0.0001  0.9998  0.3331  0.4999  0.0001]] payoff row\n",
      "[[ 0.7999  0.7999  0.7999  0.7999  0.8666  0.8999  0.8999  0.9332  0.8999  0.8999  0.8999  0.8999  0.9999  0.9999  0.9999  0.9999]\n",
      " [ 0.7999  0.7999  0.7999  0.7998  0.8665  0.7334  0.9331  0.6996  0.8998  0.7199  0.8998  0.72    0.9997  0.6665  0.9997  0.6664]\n",
      " [ 0.3669  0.3663  0.7998  0.7998  0.5252  0.6831  0.8665  0.9332  0.6198  0.6199  0.8998  0.8998  0.9999  0.9999  0.9999  0.9999]\n",
      " [ 0.15    0.2332  0.2751  0.3996  0.1835  0.3249  0.3251  0.4662  0.2003  0.3248  0.3249  0.4496  0.25    0.4162  0.3756  0.5001]\n",
      " [ 0.7998  0.7998  0.5248  0.4332  0.7998  0.65    0.7998  0.5602  0.4333  0.4333  0.2502  0.2501  0.5999  0.5     0.5999  0.5   ]\n",
      " [ 0.7998  0.7997  0.4332  0.3248  0.7997  0.3996  0.3252  0.0001  0.4333  0.325   0.25    0.1667  0.325   0.0002  0.1667  0.0001]\n",
      " [ 0.1502 -0.0664  0.7996  0.325   0.3667  0.3249  0.7997  0.56   -0.4994 -0.4995  0.325   0.1667  0.325   0.3     0.5999  0.5   ]\n",
      " [-0.0663 -0.0498 -0.0393  0.     -0.0663 -0.0329 -0.0399  0.0002 -0.4997 -0.3331 -0.2498  0.     -0.4996 -0.1665 -0.2498  0.0001]\n",
      " [ 0.1501  0.1501  0.4333  0.4333  0.3199  0.5753  0.9996  0.9997  0.4     0.5999  0.4001  0.6     0.5998  0.9998  0.6999  0.9998]\n",
      " [ 0.15    0.1201  0.4333  0.3251  0.32    0.3251  0.9996  0.6664  0.1001  0.0002  0.325   0.0003  0.3251  0.3334  0.9995  0.4999]\n",
      " [-0.4997 -0.4995  0.1001  0.325  -0.4997  0.2502  0.325   0.9996  0.0999  0.3247  0.4     0.5999  0.3251  0.9987  0.5999  0.9998]\n",
      " [-0.4998 -0.3331 -0.2498  0.0001 -0.4998 -0.0832 -0.0001  0.3334 -0.4998 -0.1666 -0.2498  0.0001 -0.4996  0.1251 -0.0002  0.3333]\n",
      " [ 0.15    0.15    0.25    0.25    0.1501  0.325   0.325   0.4999  0.3999  0.325   0.325   0.2501  0.4     0.5     0.4     0.5   ]\n",
      " [ 0.15    0.12    0.25    0.1666  0.15    0.0751  0.1667  0.0001  0.1     0.0002  0.2499  0.1251  0.1     0.0001  0.1667  0.0001]\n",
      " [-0.4997 -0.4997  0.1     0.1667 -0.4997  0.0001  0.1001  0.4999 -0.0496 -0.4995  0.3998  0.1667  0.1001  0.2997  0.3999  0.4999]\n",
      " [-0.4998 -0.3331 -0.2499  0.     -0.4998 -0.25   -0.2499  0.     -0.4998 -0.25   -0.2499  0.     -0.4998 -0.1665 -0.2499  0.0001]] payoff col\n",
      "Total number of Nash  3\n",
      "P1: ['[0.9999, 0.0001, 0.0001, 0.9999]']\n",
      "P2: ['[0.9999, 0.0001, 0.0001, 0.9999]']\n",
      "------\n",
      "P1: ['[0.9999, 0.0001, 0.0001, 0.0001]']\n",
      "P2: ['[0.9999, 0.0001, 0.0001, 0.0001]']\n",
      "------\n",
      "P1: ['[0.0001, 0.0001, 0.0001, 0.0001]']\n",
      "P2: ['[0.0001, 0.0001, 0.0001, 0.0001]']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "'''Only State Information: Calculating Nash equilibria '''\n",
    "E = 0.0001\n",
    "determinstic_strategy_itertools_no_info = itertools.product([1 - E,0 + E], repeat = 4)\n",
    "\n",
    "determinisic_strategy_lists_no_info = list(list(strat) for strat in determinstic_strategy_itertools_no_info)\n",
    "all_determinstic_strategy_dictionary_no_info = {str(strat):strat for strat in determinisic_strategy_lists_no_info}\n",
    "strategy_set_p1_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "strategy_set_p2_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "\n",
    "strategy_set_p1_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p1_no_info.items()}\n",
    "strategy_set_p2_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p2_no_info.items()}\n",
    "\n",
    "\n",
    "print(strategy_set_p1_no_info)\n",
    "\n",
    "# mode = 'only_state_information'\n",
    "# ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "\n",
    "# information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "# mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.05, discount_factors= 0)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_state_test(strategy_set_p1_no_info, strategy_set_p2_no_info, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "# payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "print(payoffs_row, \"payoff row\")\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "print(payoffs_col, \"payoff col\")\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "# print(len(result.equilibria))\n",
    " \n",
    "# for i in result.equilibria:\n",
    "#     print(i)  \n",
    "#     print(\"---\")\n",
    "\n",
    "process_and_print_nash_equilibria_results(result.equilibria, strategy_set_p1_no_info, strategy_set_p2_no_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
