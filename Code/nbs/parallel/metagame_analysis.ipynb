{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "095b105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from information_conditions import Information_Conditions\n",
    "from base_ecopg import *\n",
    "from helper_functions import *\n",
    "from simulation_and_results_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38583a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25b957bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_degraded_state_policies_both_state_and_action(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "\n",
    "        for i in [0, 2, 4, 6]:\n",
    "            x = np.random.rand()\n",
    "            strategy_propserous_and_degraded_state.insert(i, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n",
    "\n",
    "def add_degraded_state_policies_only_state(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "        x = np.random.rand()\n",
    "        strategy_propserous_and_degraded_state.insert(0, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52af6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_from_strategy(agent_1_strategy, agent_2_strategy):\n",
    "\n",
    "\n",
    "    agent_1_strategy = [[x, 1-x] for x in agent_1_strategy]\n",
    "    agent_2_strategy = [[x, 1-x] for x in agent_2_strategy]\n",
    "\n",
    "    policy = np.array([agent_1_strategy, agent_2_strategy])\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b58d1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy_obs(policy, mode):\n",
    "\n",
    "    mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    # mae_ecopg.has_last_statdist = False\n",
    "    # mae_ecopg._last_statedist = None\n",
    "\n",
    "    Vio = mae_ecopg.Vio(policy)\n",
    "    obsdist = mae_ecopg.obsdist(policy)\n",
    "\n",
    "    # print(Vio, \"Vio\")\n",
    "    # print(obsdist, \"obsdist\")\n",
    "\n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vio = Vio[:,1::2]\n",
    "        obsdist = obsdist[:,1::2]\n",
    "        obsdist = obsdist/obsdist.sum(axis = 1)\n",
    "\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "    # print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "    return avg_value_agent_1, avg_value_agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e52a781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy_state_mae_ecopg(policy, mode):\n",
    "\n",
    "    mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    Vis = mae_ecopg.Vis(policy)\n",
    "    mae_ecopg.has_last_statdist = False\n",
    "    mae_ecopg._last_statedist = None\n",
    "    state_dist = mae_ecopg._numpyPs(policy)\n",
    "\n",
    "    \n",
    "    E = 0.01\n",
    "    state_dist = np.where(state_dist == 0, E, state_dist)\n",
    "\n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "\n",
    "    state_dist = state_dist/np.sum(state_dist)\n",
    "\n",
    "    # print(Vis, \"vis\")\n",
    "    # print(state_dist, \"state dist\")\n",
    "\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "    # print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "    return avg_value_agent_1, avg_value_agent_2\n",
    "\n",
    "\n",
    "\n",
    "def calculate_avg_value_given_policy_state_test(policy, mode, mae):\n",
    "\n",
    "    Vis = mae.Vis(policy)\n",
    "    state_dist = mae.Ps(policy)\n",
    "\n",
    "    # print(Vis, \"vis\")\n",
    "    # print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "\n",
    "\n",
    "    return  avg_value_agent_1, avg_value_agent_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6629c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode):\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "    \n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_obs(policy, mode)\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_mae_ecopg(policy, mode)\n",
    "\n",
    "            # print(value_p1, value_p2 ,\"value p1, p2\")\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metagame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode, mae):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_test(policy, mode, mae)\n",
    "\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4db3dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players(mode):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    # E = 0.05\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 , 0], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1, 0], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3669c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3200d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_matrix_to_excel(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    '''p1 rewards pg 1. p2 rewards pg 2. rows -> p1 strategies, cols -> p2 strategies in both cases '''\n",
    "    p1_reward_matrix = pd.DataFrame(p1_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "    p2_reward_matrix = pd.DataFrame(p2_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "\n",
    "    with pd.ExcelWriter('reward_matrix_only_action_2.xlsx') as excel_file:\n",
    "        p1_reward_matrix.to_excel(excel_file, sheet_name='p1', index=True, header=True)\n",
    "        p2_reward_matrix.to_excel(excel_file, sheet_name='p2', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d7dd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pure_strategy_nash_equilibria(payoffs_row, payoffs_col):\n",
    "    game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "    result = pygambit.nash.enumpure_solve(game)\n",
    "    return result.equilibria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3eb89d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nstrategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\\nstrategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\\n\\nprint(strategy_set_p1_only_action_array)\\n\\nfor eq in result.equilibria:                       # each equilibrium profile\\n    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\\n\\n    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\\n    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\\n    # float_profile = np.array() # iterate over (strategy, prob\\n\\n    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\\n    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\\n\\n    print('------')\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_and_print_nash_equilibria_results(nash_equlibria_result, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    number_of_nash_equilbria = len(nash_equlibria_result)\n",
    "    print(\"Total number of Nash \", number_of_nash_equilbria)\n",
    "\n",
    "    '''better readable format of the strategies'''\n",
    "    strategies_p1_array = np.array(list(strategy_set_p1.keys())) #just the list of strategies\n",
    "    strategies_p2_array = np.array(list(strategy_set_p2.keys()))\n",
    "\n",
    "    for eq in nash_equlibria_result:                       # each equilibrium profile\n",
    "        strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]    #converting mixed strategis to float\n",
    "\n",
    "        p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool) #boolean conversion - for strategy in pure nash probability =  1, for strategy not in pure nash, probability = 0\n",
    "        p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "        # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "        p1_nash_strategy = strategies_p1_array[p1_active_strategy]\n",
    "        p2_nash_strategy =  strategies_p2_array[p2_active_strategy]\n",
    "\n",
    "        \n",
    "        print('P1:' , p1_nash_strategy)   #will only print the strategy found in nash\n",
    "        print('P2:' , p2_nash_strategy)\n",
    "\n",
    "        print('------')\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "strategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\n",
    "strategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "print(strategy_set_p1_only_action_array)\n",
    "\n",
    "for eq in result.equilibria:                       # each equilibrium profile\n",
    "    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\n",
    "\n",
    "    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\n",
    "    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "    # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\n",
    "    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\n",
    "\n",
    "    print('------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "924aefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pure_nash_equilibria_all_deterministic_strategies_mode(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players(mode)\n",
    "    p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "    # p1_reward_matrix, p2_reward_matrix = metagame_reward_matrix_obs_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "\n",
    "    print(\"p1: \\n\", p1_reward_matrix, \"\\n p2: \\n\", p2_reward_matrix)\n",
    "    \n",
    "    nash_equlibrium_result = calculate_pure_strategy_nash_equilibria(p1_reward_matrix, p2_reward_matrix)\n",
    "    process_and_print_nash_equilibria_results(nash_equlibrium_result, strategy_set_p1, strategy_set_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1166f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: \n",
      " [[  36.9797 -294.3654]\n",
      " [-287.3218 -294.3871]] \n",
      " p2: \n",
      " [[  36.9797 -287.3218]\n",
      " [-294.3654 -294.3871]]\n",
      "Total number of Nash  1\n",
      "P1: ['[1]']\n",
      "P2: ['[1]']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "get_pure_nash_equilibria_all_deterministic_strategies_mode('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66cfa859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_policy_for_stability_Q_values(policy, mode):\n",
    "    #works only for determentsic strtagues\n",
    "    mae = create_mae_ecopg_for_given_mode_POstratAC('none')\n",
    "    Qioa = mae.Qioa(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7149b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[50.   51.91]]\n",
      "\n",
      " [[50.   51.91]]]\n",
      "[[[1 0]]\n",
      "\n",
      " [[1 0]]]\n",
      "Qi [50. 50.]\n",
      "Qi_reverse [51.91 51.91]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = create_mae_ecopg_for_given_mode_POstratAC('none')\n",
    "E = 0.0\n",
    "policy = create_policy_from_strategy([1], [1])\n",
    "Qioa = mae.Qioa(policy)\n",
    "\n",
    "reverse_policy = 1 - policy\n",
    "\n",
    "print(Qioa)\n",
    "print(policy)\n",
    "Vio = mae.Vio(policy)\n",
    "# print(Qioa)\n",
    "\n",
    "obsdist = mae.obsdist(policy)\n",
    "\n",
    "# Qioa_2 =  np.random.randint(low = 0, high = 2, size = Qioa.shape)\n",
    "# print(Qioa_2, \"Qioa_2\")\n",
    "# obsdist_2 = np.random.randint(low = 0, high = 2, size = obsdist.shape)\n",
    "# print(obsdist_2, \"obsdist_2\")\n",
    "# print(obsdist)\n",
    "\n",
    "agents = 0\n",
    "actions = 1\n",
    "states = 2\n",
    "\n",
    "Qi = jnp.einsum(Qioa, [agents, states, actions],  policy, [agents, states, actions], [agents])  #Here, we're calculating the average Q value for each agent - across the policy and obsdist\n",
    "print(\"Qi\", Qi)\n",
    "\n",
    "Qi_reverse = jnp.einsum(Qioa, [agents, states, actions],  reverse_policy, [agents, states, actions], [agents])  #Here, we're calculating the average Q value for each agent - across the policy and obsdist\n",
    "print(\"Qi_reverse\", Qi_reverse)\n",
    "\n",
    "# avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "# print(avg_value_agent_1, avg_value_agent_2, \"value of agents\")\n",
    "\n",
    "\n",
    "np.all(Qi < Qi_reverse)\n",
    "# print(Qioa[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985cfdc",
   "metadata": {},
   "source": [
    "#strategy vs actions\n",
    "\n",
    "WSLS - [1, 0, 0, 1] \n",
    "Q_ioa for each. \n",
    "\n",
    "0.999\n",
    "\n",
    "value functions -> policy (Q and SA)\n",
    "policy -> actor critic\n",
    "\n",
    "\n",
    "actor critic also uses Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a19e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players_with_epsilon(mode):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    E = 0.01\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 - E , 0 + E], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1 - E , 0 + E], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(strat):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_and_print_nash_equilibria_results_test(nash_equlibria_result, strategy_set_p1, strategy_set_p2, mode, mae):\n",
    "\n",
    "    number_of_nash_equilbria = len(nash_equlibria_result)\n",
    "    print(\"Total number of Nash \", number_of_nash_equilbria)\n",
    "\n",
    "    '''better readable format of the strategies'''\n",
    "    strategies_p1_array = np.array(list(strategy_set_p1.values())) #just the list of strategies\n",
    "    strategies_p2_array = np.array(list(strategy_set_p2.values()))\n",
    "\n",
    "    for eq in nash_equlibria_result:                       # each equilibrium profile\n",
    "        strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]    #converting mixed strategis to float\n",
    "\n",
    "        p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool) #boolean conversion - for strategy in pure nash probability =  1, for strategy not in pure nash, probability = 0\n",
    "        p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "        # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "        p1_nash_strategy = (strategies_p1_array[p1_active_strategy])[0]\n",
    "        p2_nash_strategy =  (strategies_p2_array[p2_active_strategy])[0]\n",
    "\n",
    "        \n",
    "        print('P1:' , p1_nash_strategy)   #will only print the strategy found in nash\n",
    "        print('P2:' , p2_nash_strategy)\n",
    "\n",
    "        policy_test = create_policy_from_strategy(p1_nash_strategy, p2_nash_strategy)\n",
    "\n",
    "        print(calculate_avg_value_given_policy_state_test_with_print(policy_test, mode, mae))\n",
    "\n",
    "        print('------')\n",
    "\n",
    "\n",
    "def calculate_avg_value_given_policy_state_test_with_print(policy, mode, mae):\n",
    "\n",
    "    Vis = mae.Vis(policy)\n",
    "    state_dist = mae.Ps(policy)\n",
    "\n",
    "    print(Vis, \"vis\")\n",
    "    print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "\n",
    "    print(avg_value_agent_1, avg_value_agent_2, \"Vi\")\n",
    "\n",
    "    return  avg_value_agent_1, avg_value_agent_2\n",
    "\n",
    "\n",
    "'''Memo1pd testing for understanding '''\n",
    "\n",
    "\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "\n",
    "\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "\n",
    "\n",
    "\n",
    "process_and_print_nash_equilibria_results_test(result.equilibria, strategy_set_p1, strategy_set_p2, mode, mae_socdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd252aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Nash  3\n",
      "P1: [0.99 0.01 0.01 0.99]\n",
      "P2: [0.99 0.01 0.01 0.99]\n",
      "[[77.3666 76.5826 76.5825 77.3665]\n",
      " [77.3666 76.5826 76.5825 77.3665]] vis\n",
      "[0.9607 0.0099 0.0099 0.0195] state dist\n",
      "77.35103 77.35103 Vi\n",
      "(Array(77.351, dtype=float32), Array(77.351, dtype=float32))\n",
      "------\n",
      "P1: [0.99 0.01 0.01 0.01]\n",
      "P2: [0.99 0.01 0.01 0.01]\n",
      "[[27.0722  0.7635  0.7635  0.7635]\n",
      " [27.0722  0.7635  0.7635  0.7635]] vis\n",
      "[0.005  0.0099 0.0099 0.9752] state dist\n",
      "0.8950479 0.89504796 Vi\n",
      "(Array(0.895, dtype=float32), Array(0.895, dtype=float32))\n",
      "------\n",
      "P1: [0.01 0.01 0.01 0.01]\n",
      "P2: [0.01 0.01 0.01 0.01]\n",
      "[[0.503 0.503 0.503 0.503]\n",
      " [0.503 0.503 0.503 0.503]] vis\n",
      "[0.0001 0.0099 0.0099 0.9801] state dist\n",
      "0.5030022 0.5030023 Vi\n",
      "(Array(0.503, dtype=float32), Array(0.503, dtype=float32))\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "'''Memo1pd testing for understanding '''\n",
    "\n",
    "\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env_socdi = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "env_memo1pd = HistoryEmbedded(env_socdi, h=(1,1,1))\n",
    "\n",
    "strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players_with_epsilon(mode)\n",
    "\n",
    "\n",
    "mae_socdi = stratAC(env=env_memo1pd, learning_rates=0.1, discount_factors= 0.99)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metagame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "\n",
    "\n",
    "\n",
    "process_and_print_nash_equilibria_results_test(result.equilibria, strategy_set_p1, strategy_set_p2, mode, mae_socdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51f603",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3447455901.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_q_value(Qioa, obsdist, policy):\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
