{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "095b105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from information_conditions import Information_Conditions\n",
    "from base_ecopg import *\n",
    "from helper_functions import *\n",
    "from simulation_and_results_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "38583a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "25b957bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_degraded_state_policies_both_state_and_action(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "\n",
    "        for i in [0, 2, 4, 6]:\n",
    "            x = np.random.rand()\n",
    "            strategy_propserous_and_degraded_state.insert(i, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n",
    "\n",
    "def add_degraded_state_policies_only_state(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "        x = np.random.rand()\n",
    "        strategy_propserous_and_degraded_state.insert(0, x)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "52af6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_from_strategy(agent_1_strategy, agent_2_strategy):\n",
    "\n",
    "\n",
    "    agent_1_strategy = [[x, 1-x] for x in agent_1_strategy]\n",
    "    agent_2_strategy = [[x, 1-x] for x in agent_2_strategy]\n",
    "\n",
    "    policy = np.array([agent_1_strategy, agent_2_strategy])\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b58d1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy(policy, mae, mode):\n",
    "    Vio = mae.Vio(policy)\n",
    "    obsdist = mae.obsdist(policy)\n",
    "\n",
    "    # if mode == 'both_state_and_action_information' or mode == 'only_state_information':\n",
    "    #     Vio = Vio[:,1::2]\n",
    "    #     obsdist = obsdist[:,1::2]\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "    return avg_value_agent_1, avg_value_agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e52a781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy_state_mae_ecopg(policy, mode):\n",
    "\n",
    "    mae_ecopg =  create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    Vis = mae_ecopg.Vis(policy)\n",
    "    mae_ecopg.has_last_statdist = False\n",
    "    mae_ecopg._last_statedist = None\n",
    "    state_dist = mae_ecopg._numpyPs(policy)\n",
    "\n",
    "    # print(Vis, \"vis\")\n",
    "    # print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "    return  avg_value_agent_1, avg_value_agent_2\n",
    "\n",
    "\n",
    "def calculate_avg_value_given_policy_state_test(policy, mode, mae):\n",
    "\n",
    "    Vis = mae.Vis(policy)\n",
    "    state_dist = mae.Ps(policy)\n",
    "\n",
    "    # print(Vis, \"vis\")\n",
    "    # print(state_dist, \"state dist\")\n",
    "    \n",
    "    if mode == 'complete' or mode == 'ecological':\n",
    "        Vis = Vis[:,1::2]\n",
    "        state_dist = state_dist[1::2]\n",
    "        state_dist = state_dist/np.sum(state_dist)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value_agent_1, avg_value_agent_2 = jnp.einsum(Vis, [agents, states], state_dist , [states], [agents])\n",
    "    return  avg_value_agent_1, avg_value_agent_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6629c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metgame_reward_matrix(strategy_set_p1, strategy_set_p2, mae, mode):\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "    \n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy(policy, mae, mode)\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metgame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_mae_ecopg(policy, mode)\n",
    "\n",
    "            # print(value_p1, value_p2 ,\"value p1, p2\")\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n",
    "\n",
    "\n",
    "\n",
    "def metgame_reward_matrix_state_test(strategy_set_p1, strategy_set_p2, mode, mae):\n",
    "    '''Vi estimate using vis and Ps instead of Vio and obsdist'''\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "\n",
    "            # print(p1_strategy, p2_strategy, \"strat p1, p2\")\n",
    "\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            value_p1, value_p2 = calculate_avg_value_given_policy_state_test(policy, mode, mae)\n",
    "\n",
    "            # print(value_p1, value_p2 ,\"value p1, p2\")\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4db3dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinstic_strategies_set_for_both_players(mode):\n",
    "    '''creates deterministic strategy sets for given mode '''\n",
    "\n",
    "    mae_ecopg_for_evaluating_no_of_states = create_mae_ecopg_for_given_mode_POstratAC(mode)\n",
    "    number_of_states = mae_ecopg_for_evaluating_no_of_states.Q\n",
    "    \n",
    "    if mode == 'none' or mode == 'social':\n",
    "        determinstic_strategy_itertools = itertools.product([1 , 0], repeat = number_of_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_full = {str(np.round(strat)):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "    else:\n",
    "        number_of_prosperous_states = int(number_of_states/2)\n",
    "        determinstic_strategy_itertools = itertools.product([1 , 0], repeat = number_of_prosperous_states)\n",
    "        determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "        all_determinstic_strategy_dictionary_only_prosperous = {str(np.round(strat)):strat for strat in determinisic_strategy_lists}\n",
    "\n",
    "        if mode == 'complete':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "        elif mode == 'ecological':\n",
    "             all_determinstic_strategy_dictionary_full = {key: add_degraded_state_policies_only_state(value) for key, value in all_determinstic_strategy_dictionary_only_prosperous.items()}\n",
    "\n",
    "\n",
    "    strategy_set_p1 = all_determinstic_strategy_dictionary_full\n",
    "    strategy_set_p2 = all_determinstic_strategy_dictionary_full\n",
    "\n",
    "    return strategy_set_p1, strategy_set_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a3200d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_matrix_to_excel(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    '''p1 rewards pg 1. p2 rewards pg 2. rows -> p1 strategies, cols -> p2 strategies in both cases '''\n",
    "    p1_reward_matrix = pd.DataFrame(p1_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "    p2_reward_matrix = pd.DataFrame(p2_reward_matrix, index = list(strategy_set_p1.keys()), columns= list(strategy_set_p2.keys()))\n",
    "\n",
    "    with pd.ExcelWriter('reward_matrix_only_action_2.xlsx') as excel_file:\n",
    "        p1_reward_matrix.to_excel(excel_file, sheet_name='p1', index=True, header=True)\n",
    "        p2_reward_matrix.to_excel(excel_file, sheet_name='p2', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d7dd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pure_strategy_nash_equilibria(payoffs_row, payoffs_col):\n",
    "    game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "    result = pygambit.nash.enumpure_solve(game)\n",
    "    return result.equilibria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3eb89d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nstrategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\\nstrategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\\n\\nprint(strategy_set_p1_only_action_array)\\n\\nfor eq in result.equilibria:                       # each equilibrium profile\\n    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\\n\\n    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\\n    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\\n    # float_profile = np.array() # iterate over (strategy, prob\\n\\n    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\\n    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\\n\\n    print('------')\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_and_print_nash_equilibria_results(nash_equlibria_result, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    number_of_nash_equilbria = len(nash_equlibria_result)\n",
    "    print(\"Total number of Nash \", number_of_nash_equilbria)\n",
    "\n",
    "    '''better readable format of the strategies'''\n",
    "    strategies_p1_array = np.array(list(strategy_set_p1.keys())) #just the list of strategies\n",
    "    strategies_p2_array = np.array(list(strategy_set_p2.keys()))\n",
    "\n",
    "    for eq in nash_equlibria_result:                       # each equilibrium profile\n",
    "        strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]    #converting mixed strategis to float\n",
    "\n",
    "        p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool) #boolean conversion - for strategy in pure nash probability =  1, for strategy not in pure nash, probability = 0\n",
    "        p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "        # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "        print('P1:' , strategies_p1_array[p1_active_strategy])   #will only print the strategy found in nash\n",
    "        print('P2:' , strategies_p2_array[p2_active_strategy])\n",
    "\n",
    "        print('------')\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "strategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\n",
    "strategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "print(strategy_set_p1_only_action_array)\n",
    "\n",
    "for eq in result.equilibria:                       # each equilibrium profile\n",
    "    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\n",
    "\n",
    "    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\n",
    "    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "    # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\n",
    "    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\n",
    "\n",
    "    print('------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "924aefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pure_nash_equilibria_all_deterministic_strategies_mode(mode):\n",
    "    strategy_set_p1, strategy_set_p2 = create_determinstic_strategies_set_for_both_players(mode)\n",
    "    p1_reward_matrix, p2_reward_matrix = metgame_reward_matrix_state_ecopg(strategy_set_p1, strategy_set_p2, mode)\n",
    "\n",
    "    print(\"p1: \\n\", p1_reward_matrix, \"\\n p2: \\n\", p1_reward_matrix)\n",
    "    \n",
    "    nash_equlibrium_result = calculate_pure_strategy_nash_equilibria(p1_reward_matrix, p2_reward_matrix)\n",
    "    process_and_print_nash_equilibria_results(nash_equlibrium_result, strategy_set_p1, strategy_set_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1166f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: \n",
      " [[  50.     -298.0397]\n",
      " [-295.5892 -298.5374]] \n",
      " p2: \n",
      " [[  50.     -298.0397]\n",
      " [-295.5892 -298.5374]]\n",
      "Total number of Nash  1\n",
      "P1: ['[1]']\n",
      "P2: ['[1]']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "get_pure_nash_equilibria_all_deterministic_strategies_mode('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "054f3a78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy_set_p1_only_action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[152]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m mode = \u001b[33m'\u001b[39m\u001b[33monly_action_history_information\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m p1_reward_matrix_only_action, p2_reward_matrix_only_action = metgame_reward_matrix_state(\u001b[43mstrategy_set_p1_only_action\u001b[49m, strategy_set_p2_only_action, mode)\n\u001b[32m      8\u001b[39m payoffs_row =  p1_reward_matrix_only_action\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'strategy_set_p1_only_action' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "mode = 'only_action_history_information'\n",
    "\n",
    "p1_reward_matrix_only_action, p2_reward_matrix_only_action = metgame_reward_matrix_state(strategy_set_p1_only_action, strategy_set_p2_only_action, mode)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "payoffs_row =  p1_reward_matrix_only_action\n",
    "# payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "\n",
    "print(payoffs_row)\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_only_action\n",
    "\n",
    "print(payoffs_col)\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "\n",
    "print(len(result.equilibria))\n",
    " \n",
    "# for i in result.equilibria:\n",
    "#     print(i)  \n",
    "#     print(\"---\")\n",
    "strategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\n",
    "strategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "print(strategy_set_p1_only_action)\n",
    "\n",
    "for eq in result.equilibria:                       # each equilibrium profile\n",
    "    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\n",
    "\n",
    "    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\n",
    "    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "    # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\n",
    "    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\n",
    "\n",
    "    print('------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[1]': [1], '[0]': [0]}\n",
      "[[  50.     -298.0397]\n",
      " [-295.5892 -298.5374]]\n",
      "[[  50.     -295.5892]\n",
      " [-298.0397 -298.5374]]\n",
      "1\n",
      "[[Rational(1, 1), Rational(0, 1)], [Rational(1, 1), Rational(0, 1)]]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "'''No Information: Calculating Nash equilibria '''\n",
    "\n",
    "\n",
    "determinstic_strategy_itertools_no_info = itertools.product([1,0], repeat = 1)\n",
    "\n",
    "determinisic_strategy_lists_no_info = list(list(strat) for strat in determinstic_strategy_itertools_no_info)\n",
    "all_determinstic_strategy_dictionary_no_info = {str(strat):strat for strat in determinisic_strategy_lists_no_info}\n",
    "strategy_set_p1_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "strategy_set_p2_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "\n",
    "print(strategy_set_p1_no_info)\n",
    "\n",
    "mode = 'none'\n",
    "ecopg = BaseEcologicalPublicGood(qc = 0)\n",
    "\n",
    "information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "\n",
    "p1_reward_matrix_no_info, p2_reward_matrix_no_info = metgame_reward_matrix_state(strategy_set_p1_no_info, strategy_set_p2_no_info, mode)\n",
    "\n",
    "\n",
    "p1_reward_matrix_only_action_df = pd.DataFrame(p1_reward_matrix_no_info, index = list(strategy_set_p1_no_info.keys()), columns= list(strategy_set_p1_no_info.keys()))\n",
    "p2_reward_matrix_only_action_df = pd.DataFrame(p2_reward_matrix_no_info, index = list(strategy_set_p1_no_info.keys()), columns= list(strategy_set_p2_no_info.keys()))\n",
    "\n",
    "with pd.ExcelWriter('reward_matrix_no_info.xlsx') as excel_file:\n",
    "    p1_reward_matrix_only_action_df.to_excel(excel_file, sheet_name='p1_reward_matrix', index=True, header=True)\n",
    "    p2_reward_matrix_only_action_df.to_excel(excel_file, sheet_name='p2_reward_matrix', index=True, header=True)\n",
    "\n",
    "    \n",
    "payoffs_row = p1_reward_matrix_no_info\n",
    "print(payoffs_row)\n",
    "payoffs_col  = p2_reward_matrix_no_info\n",
    "print(payoffs_col)\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "print(len(result.equilibria))\n",
    " \n",
    "for i in result.equilibria:\n",
    "    print(i)  \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34722738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.       0.9817   2.9482   2.9482   0.       0.9817   2.9482   2.9482   0.       0.9817   2.9482   2.9482   0.       0.9817   2.9482   2.9482]\n",
      " [  0.1647   1.7055   2.9482   2.9482   0.8622   1.7055   2.9482   2.9482   0.1647   2.1212   2.9482   2.9482   0.8622   2.1212   2.9482   2.9482]\n",
      " [  0.       0.8599   0.       2.1185   1.7226   1.7226   0.       2.1185   0.       0.8599   6.3225   6.3225   1.7226   1.7226   6.3225   6.3225]\n",
      " [  0.1647   1.7055   0.1647   1.7055   1.7226   1.7249   1.7152   1.7055   0.1647   1.7155   6.3282   6.3226   1.7226   1.7226   6.3225   6.3226]\n",
      " [  0.4977   0.5047   0.4977   0.4977   0.       0.9817   2.9482   2.9482   0.4977   0.5047   0.4977   0.4977   0.       0.9817   2.9482   2.9482]\n",
      " [  0.4977   0.4977   0.4977   2.5669   1.307    1.7055   2.948    2.9482   0.4977   0.8967   0.4977   0.4977   1.7189   2.1212   2.9482   2.9482]\n",
      " [  0.4977   0.4977   0.4977   0.8177   0.       1.7155   0.       2.1185   0.       0.4977   0.       6.3133   0.       3.4248   6.3225   6.3225]\n",
      " [  0.4977   0.4977   0.4977   0.4977   1.307    1.7055   1.307    1.7055   0.4977   0.4977   0.4977   0.4977   3.4247   3.4248   6.3225   6.3226]\n",
      " [  0.       0.9811   2.9482   2.9482   0.       0.9811   2.9482   2.9482   0.0001 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374]\n",
      " [  0.1647   1.3046   2.9482   2.9464   0.8622   1.715    2.9482   2.9482 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374]\n",
      " [  0.       0.8599   0.       1.7127   1.7226   1.7226   0.       3.4217   0.0001 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374]\n",
      " [  0.1647   1.3046   0.1647   1.3046   1.7226   1.7226   3.4217   3.4217 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374]\n",
      " [  0.4977   0.5047   0.4977   0.4977   0.       0.9817   2.9482   2.9482   0.4977 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374 348.5374]\n",
      " [  0.4977   0.4977   0.4977   0.4977   3.8949   3.8949   2.9482   2.9482   0.4977   0.4979   0.4977   0.4977 348.5374 348.5374 348.5374 348.5374]\n",
      " [  0.4977   0.4977   0.4977   0.4977   0.       3.8949   0.       3.8949   0.4977   0.4979   0.4977   0.4979   0.0001 348.5374 348.5374 348.5374]\n",
      " [  0.4977   0.4977   0.4977   0.4977   3.8949   3.8949   3.8949   3.8949   0.4979   0.4979   0.4979   0.4979 348.5374 348.5374 348.5374 348.5374]]\n",
      "12\n",
      "['[0, 0, 0, 0]' '[0, 0, 0, 1]' '[0, 0, 1, 0]' '[0, 0, 1, 1]' '[0, 1, 0, 0]' '[0, 1, 0, 1]' '[0, 1, 1, 0]' '[0, 1, 1, 1]' '[1, 0, 0, 0]' '[1, 0, 0, 1]' '[1, 0, 1, 0]' '[1, 0, 1, 1]' '[1, 1, 0, 0]'\n",
      " '[1, 1, 0, 1]' '[1, 1, 1, 0]' '[1, 1, 1, 1]']\n",
      "P1: ['[1, 0, 0, 1]']\n",
      "P2: ['[1, 0, 0, 0]']\n",
      "------\n",
      "P1: ['[1, 0, 1, 1]']\n",
      "P2: ['[1, 0, 0, 0]']\n",
      "------\n",
      "P1: ['[1, 0, 0, 0]']\n",
      "P2: ['[1, 0, 0, 1]']\n",
      "------\n",
      "P1: ['[1, 0, 1, 0]']\n",
      "P2: ['[1, 0, 0, 1]']\n",
      "------\n",
      "P1: ['[1, 1, 0, 0]']\n",
      "P2: ['[1, 0, 0, 1]']\n",
      "------\n",
      "P1: ['[1, 0, 0, 1]']\n",
      "P2: ['[1, 0, 1, 0]']\n",
      "------\n",
      "P1: ['[1, 1, 0, 0]']\n",
      "P2: ['[1, 0, 1, 0]']\n",
      "------\n",
      "P1: ['[1, 0, 0, 0]']\n",
      "P2: ['[1, 0, 1, 1]']\n",
      "------\n",
      "P1: ['[1, 1, 0, 0]']\n",
      "P2: ['[1, 0, 1, 1]']\n",
      "------\n",
      "P1: ['[1, 0, 0, 1]']\n",
      "P2: ['[1, 1, 0, 0]']\n",
      "------\n",
      "P1: ['[1, 0, 1, 0]']\n",
      "P2: ['[1, 1, 0, 0]']\n",
      "------\n",
      "P1: ['[1, 0, 1, 1]']\n",
      "P2: ['[1, 1, 0, 0]']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "       # pygambit is imported as “gambit\n",
    "\n",
    "payoffs_row =  p1_reward_matrix_only_action\n",
    "payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "\n",
    "print(payoffs_row_modifed)\n",
    "\n",
    "payoffs_col  = np.transpose(p2_reward_matrix_only_action)\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "\n",
    "print(len(result.equilibria))\n",
    " \n",
    "# for i in result.equilibria:\n",
    "#     print(i)  \n",
    "#     print(\"---\")\n",
    "strategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\n",
    "strategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "print(strategy_set_p1_only_action_array)\n",
    "\n",
    "for eq in result.equilibria:                       # each equilibrium profile\n",
    "    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\n",
    "\n",
    "    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\n",
    "    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "    # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\n",
    "    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\n",
    "\n",
    "    print('------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86936f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_set_p1_both_state_and_action = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in strategy_set_p1_only_action.items()}\n",
    "strategy_set_p2_both_state_and_action = {key: add_degraded_state_policies_both_state_and_action(value) for key, value in strategy_set_p2_only_action.items()}\n",
    "\n",
    "\n",
    "mode = 'both_state_and_action_information'\n",
    "ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "\n",
    "p1_reward_matrix_both_state_and_action, p2_reward_matrix_both_state_and_action = metgame_reward_matrix_state(strategy_set_p1_both_state_and_action, strategy_set_p2_both_state_and_action, mae_ecopg, mode)\n",
    "       \n",
    "\n",
    "# p1_reward_matrix_both_state_and_action_df = pd.DataFrame(p1_reward_matrix_both_state_and_action, index = list(strategy_set_p1_both_state_and_action.keys()), columns= list(strategy_set_p2_both_state_and_action.keys()))\n",
    "# p2_reward_matrix_both_state_and_action_df = pd.DataFrame(p2_reward_matrix_both_state_and_action, index = list(strategy_set_p2_both_state_and_action.keys()), columns= list(strategy_set_p2_both_state_and_action.keys()))\n",
    "\n",
    "# with pd.ExcelWriter('reward_matrix_both_state_and_action.xlsx') as excel_file:\n",
    "#     p1_reward_matrix_both_state_and_action_df.to_excel(excel_file, sheet_name='p1_reward_matrix', index=True, header=True)\n",
    "#     p2_reward_matrix_both_state_and_action_df.to_excel(excel_file, sheet_name='p2_reward_matrix', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5351de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.        0.        0.        0.        0.        0.        0.        0.     -129.9658 -129.9729 -129.9667 -129.9738 -216.8567 -216.8956 -216.8618 -216.9008]\n",
      " [   0.        0.       -0.       -0.        0.        0.        0.        0.     -129.9498 -163.5421 -129.9677 -163.592  -216.7801 -206.825  -216.8559 -206.8178]\n",
      " [   0.        0.        0.       -0.        0.        0.        0.        0.     -125.6577 -125.7305 -129.9669 -129.9762 -199.8804 -199.786  -216.8518 -216.9001]\n",
      " [   0.        0.        0.        0.        0.        0.        0.        0.      -67.4078 -157.2906 -134.7205 -176.6425  -69.8961 -178.7498 -150.0812 -204.5439]\n",
      " [   0.        0.        0.        0.        0.        0.        0.        0.     -100.3094 -100.3124 -133.7057 -133.7202 -161.6872 -207.8942 -161.6895 -207.9013]\n",
      " [   0.        0.        0.        0.        0.        0.        0.        0.     -100.2994 -155.1654 -133.7072 -176.9053 -134.052  -201.149  -160.7825 -201.1573]\n",
      " [   0.        0.        0.        0.        0.        0.        0.        0.      -50.6001  -50.5721 -133.8486 -160.6158 -150.2647 -184.4234 -161.6875 -207.901 ]\n",
      " [   0.        0.        0.        0.        0.        0.        0.        0.      -50.5272 -146.9921 -136.6241 -201.1433  -50.5361 -173.0722 -136.6275 -201.1562]\n",
      " [ -30.2753  -30.3001 -100.0906 -100.1027  -40.3875 -177.2464 -216.7791 -216.9001 -133.9896 -161.5023 -133.9911 -161.5026 -142.52   -216.8942 -151.0497 -216.9007]\n",
      " [ -30.2769 -110.7796 -100.1021 -155.0658  -40.3431 -149.8838 -216.8552 -206.8162 -106.2876 -201.1375 -133.8492 -201.1307 -117.6708 -204.0788 -216.8545 -206.5276]\n",
      " [ -50.5163  -50.5533 -106.0994 -133.6358  -67.4988 -197.5313 -117.4851 -216.8992 -125.4059 -150.0805 -133.9914 -161.5045 -133.9326 -199.7836 -142.5288 -216.9   ]\n",
      " [ -50.5253 -146.8333 -136.6171 -201.1262  -67.6413 -175.0556 -148.6972 -204.0908  -50.5393 -173.1133 -136.6238 -201.1468  -67.6446 -178.1458 -148.5954 -204.1889]\n",
      " [ -30.2755  -30.3043 -133.6735 -133.7201  -30.2792 -157.8211 -133.8434 -207.9002 -133.9853 -133.8449 -133.9328 -133.7209 -133.9915 -207.8935 -133.9975 -207.9012]\n",
      " [ -30.2772 -110.8792 -133.707  -176.9073  -30.2814 -142.1651 -160.7821 -201.1561 -106.2855 -201.1195 -133.7074 -177.6929 -106.2882 -201.1468 -160.7822 -201.1572]\n",
      " [ -50.5174  -50.5714 -106.1018 -160.6159  -50.5223 -181.5232 -106.1019 -207.8998 -116.8243  -50.5737 -133.9916 -160.6163 -125.4007 -184.4323 -133.9932 -207.9009]\n",
      " [ -50.5265 -146.9898 -136.6238 -201.1444  -50.5336 -168.7468 -136.6264 -201.1547  -50.5275 -149.767  -136.6242 -201.1458  -50.5346 -172.1461 -136.6267 -201.1561]]\n",
      "32\n",
      "['[1 1 1 1]' '[1 1 1 0]' '[1 1 0 1]' '[1 1 0 0]' '[1 0 1 1]' '[1 0 1 0]' '[1 0 0 1]' '[1 0 0 0]' '[0 1 1 1]' '[0 1 1 0]' '[0 1 0 1]' '[0 1 0 0]' '[0 0 1 1]' '[0 0 1 0]' '[0 0 0 1]' '[0 0 0 0]']\n",
      "P1: ['[1 1 1 0]']\n",
      "P2: ['[1 1 1 1]']\n",
      "------\n",
      "P1: ['[1 0 1 0]']\n",
      "P2: ['[1 1 1 1]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 1 1 1]']\n",
      "------\n",
      "P1: ['[1 1 1 1]']\n",
      "P2: ['[1 1 1 0]']\n",
      "------\n",
      "P1: ['[1 0 0 1]']\n",
      "P2: ['[1 1 1 0]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 1 1 0]']\n",
      "------\n",
      "P1: ['[1 0 1 0]']\n",
      "P2: ['[1 1 0 1]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 1 0 1]']\n",
      "------\n",
      "P1: ['[1 0 1 1]']\n",
      "P2: ['[1 1 0 0]']\n",
      "------\n",
      "P1: ['[1 0 1 0]']\n",
      "P2: ['[1 1 0 0]']\n",
      "------\n",
      "P1: ['[1 0 0 1]']\n",
      "P2: ['[1 1 0 0]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 1 0 0]']\n",
      "------\n",
      "P1: ['[1 1 0 0]']\n",
      "P2: ['[1 0 1 1]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 0 1 1]']\n",
      "------\n",
      "P1: ['[1 1 0 1]']\n",
      "P2: ['[1 0 1 0]']\n",
      "------\n",
      "P1: ['[1 1 0 0]']\n",
      "P2: ['[1 0 1 0]']\n",
      "------\n",
      "P1: ['[1 0 0 1]']\n",
      "P2: ['[1 0 1 0]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 0 1 0]']\n",
      "------\n",
      "P1: ['[1 1 1 0]']\n",
      "P2: ['[1 0 0 1]']\n",
      "------\n",
      "P1: ['[1 1 0 0]']\n",
      "P2: ['[1 0 0 1]']\n",
      "------\n",
      "P1: ['[1 0 1 0]']\n",
      "P2: ['[1 0 0 1]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 0 0 1]']\n",
      "------\n",
      "P1: ['[1 1 1 1]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[1 1 1 0]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[1 1 0 1]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[1 1 0 0]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[1 0 1 1]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[1 0 1 0]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[1 0 0 1]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[1 0 0 0]']\n",
      "P2: ['[1 0 0 0]']\n",
      "------\n",
      "P1: ['[0 0 0 1]']\n",
      "P2: ['[0 0 0 1]']\n",
      "------\n",
      "P1: ['[0 0 0 0]']\n",
      "P2: ['[0 0 0 0]']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "       # pygambit is imported as “gambit\n",
    "\n",
    "payoffs_row =  p1_reward_matrix_both_state_and_action\n",
    "payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "\n",
    "print(payoffs_row_modifed)\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_both_state_and_action\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "\n",
    "print(len(result.equilibria))\n",
    " \n",
    "# for i in result.equilibria:\n",
    "#     print(i)  \n",
    "#     print(\"---\")\n",
    "strategy_set_p1_only_action_array = np.array(list(strategy_set_p1_only_action.keys()))\n",
    "strategy_set_p2_only_action_array = np.array(list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "print(strategy_set_p1_only_action_array)\n",
    "\n",
    "for eq in result.equilibria:                       # each equilibrium profile\n",
    "    strategy_profile_of_each_agent = [[float(p) for _, p in m] for r, m in eq.mixed_strategies()]\n",
    "\n",
    "    p1_active_strategy = np.array(strategy_profile_of_each_agent[0]).astype(bool)\n",
    "    p2_active_strategy = np.array(strategy_profile_of_each_agent[1]).astype(bool)\n",
    "    # float_profile = np.array() # iterate over (strategy, prob\n",
    "\n",
    "    print('P1:' , strategy_set_p1_only_action_array[p1_active_strategy])\n",
    "    print('P2:' , strategy_set_p2_only_action_array[p2_active_strategy])\n",
    "\n",
    "    print('------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[1]': [0.03128370263515068, 1], '[0]': [0.625484167963071, 0]}\n",
      "[[  50.     -298.0411]\n",
      " [-295.5898 -298.5377]]\n",
      "[[  50.     -295.5915]\n",
      " [-298.0399 -298.5377]]\n",
      "1\n",
      "[[Rational(1, 1), Rational(0, 1)], [Rational(1, 1), Rational(0, 1)]]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "'''Only State Information: Calculating Nash equilibria '''\n",
    "\n",
    "epsilon = 0\n",
    "# determinstic_strategy_itertools_no_info = itertools.product([0,1], repeat = 1)\n",
    "determinstic_strategy_itertools_no_info = itertools.product([1,0], repeat = 1)\n",
    "\n",
    "determinisic_strategy_lists_no_info = list(list(strat) for strat in determinstic_strategy_itertools_no_info)\n",
    "all_determinstic_strategy_dictionary_no_info = {str(strat):strat for strat in determinisic_strategy_lists_no_info}\n",
    "strategy_set_p1_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "strategy_set_p2_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "\n",
    "strategy_set_p1_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p1_no_info.items()}\n",
    "strategy_set_p2_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p2_no_info.items()}\n",
    "\n",
    "print(strategy_set_p1_both_only_state)\n",
    "\n",
    "mode = 'ecological'\n",
    "ecopg = BaseEcologicalPublicGood(m = -6)\n",
    "\n",
    "information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metgame_reward_matrix_state(strategy_set_p1_both_only_state, strategy_set_p2_both_only_state,mode)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "print(payoffs_row)\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "print(payoffs_col)\n",
    "game_only_state = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result_only_state = pygambit.nash.enumpure_solve(game_only_state)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "print(len(result_only_state.equilibria))\n",
    " \n",
    "for i in result_only_state.equilibria:\n",
    "    print(i)  \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a795fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[1]': [1], '[0]': [0]}\n",
      "[[ 0.8 -0.5]\n",
      " [ 1.   0. ]] payoff row\n",
      "[[ 0.8  1. ]\n",
      " [-0.5  0. ]] payoff col\n",
      "1\n",
      "[[Rational(0, 1), Rational(1, 1)], [Rational(0, 1), Rational(1, 1)]]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "'''Only State Information: Calculating Nash equilibria '''\n",
    "\n",
    "determinstic_strategy_itertools_no_info = itertools.product([1,0], repeat = 1)\n",
    "\n",
    "determinisic_strategy_lists_no_info = list(list(strat) for strat in determinstic_strategy_itertools_no_info)\n",
    "all_determinstic_strategy_dictionary_no_info = {str(strat):strat for strat in determinisic_strategy_lists_no_info}\n",
    "strategy_set_p1_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "strategy_set_p2_no_info = all_determinstic_strategy_dictionary_no_info\n",
    "\n",
    "strategy_set_p1_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p1_no_info.items()}\n",
    "strategy_set_p2_both_only_state = {key: add_degraded_state_policies_only_state(value) for key, value in strategy_set_p2_no_info.items()}\n",
    "\n",
    "\n",
    "print(strategy_set_p1_no_info)\n",
    "\n",
    "# mode = 'only_state_information'\n",
    "# ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "mode = 'social'\n",
    "\n",
    "env = SocialDilemma(R=0.8, T=1, S=-0.5, P= 0)\n",
    "\n",
    "\n",
    "\n",
    "# information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "# mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "mae_socdi = stratAC(env=env, learning_rates=0.05, discount_factors= 0)\n",
    "\n",
    "p1_reward_matrix_only_state, p2_reward_matrix_only_state = metgame_reward_matrix_state_test(strategy_set_p1_no_info, strategy_set_p2_no_info, mode,mae_socdi)\n",
    "\n",
    "payoffs_row = p1_reward_matrix_only_state\n",
    "# payoffs_row_modifed = payoffs_row - payoffs_row[0,0]\n",
    "print(payoffs_row, \"payoff row\")\n",
    "\n",
    "payoffs_col  = p2_reward_matrix_only_state\n",
    "\n",
    "print(payoffs_col, \"payoff col\")\n",
    "\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "print(len(result.equilibria))\n",
    " \n",
    "for i in result.equilibria:\n",
    "    print(i)  \n",
    "    print(\"---\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
