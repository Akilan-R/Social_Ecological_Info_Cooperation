{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095b105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from information_conditions import Information_Conditions\n",
    "from base_ecopg import BaseEcologicalPublicGood\n",
    "from helper_functions import *\n",
    "from simulation_and_results_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58d1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_value_given_policy(policy, mae):\n",
    "    Vio = mae.Vio(policy)\n",
    "    obsdist = mae.obsdist(policy)\n",
    "    agents = 0\n",
    "    states = 1\n",
    "    avg_value = jnp.einsum(Vio, [agents, states], obsdist , [agents, states], [agents])\n",
    "    return avg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9aa6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a16443",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_determinisic_strategy = list(list(strat) for strat in itertools.product([0,1], repeat = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSLS = [1,0,0,1]\n",
    "GT = [1,0,0,0]\n",
    "ALLC = [1,1,1,1]\n",
    "TFT = [1,0,1,0]\n",
    "\n",
    "ALLD = [0,0,0,0]\n",
    "ReverseGT = [0,0,0,1]\n",
    "\n",
    "strategy_set_p1_only_action =  {\n",
    "    'WSLS': WSLS,\n",
    "    'GT': GT,\n",
    "    'ALLC': ALLC,\n",
    "    'ALLD': ALLD,\n",
    "    'ReverseGT': ReverseGT\n",
    "}\n",
    "strategy_set_p2_only_action =  {\n",
    "    'WSLS': WSLS,\n",
    "    'GT': GT,\n",
    "    'ALLC': ALLC,\n",
    "    'ALLD': ALLD,\n",
    "    'ReverseGT': ReverseGT\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b15f7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_degraded_state_policies(strategy):\n",
    "        \n",
    "        strategy_propserous_and_degraded_state = strategy.copy()\n",
    "\n",
    "        for i in [0, 2, 4, 6]:\n",
    "\n",
    "            strategy_propserous_and_degraded_state.insert(i, 0.5)\n",
    "        \n",
    "        return strategy_propserous_and_degraded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf272336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_from_strategy(agent_1_strategy, agent_2_strategy):\n",
    "\n",
    "\n",
    "    agent_1_strategy = [[x, 1-x] for x in agent_1_strategy]\n",
    "    agent_2_strategy = [[x, 1-x] for x in agent_2_strategy]\n",
    "\n",
    "    policy = np.array([agent_1_strategy, agent_2_strategy])\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6629c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metgame_reward_matrix(strategy_set_p1, strategy_set_p2, mae):\n",
    "    p1_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "    p2_reward_matrix = np.zeros((len(strategy_set_p1), len(strategy_set_p2)))\n",
    "\n",
    "    for i, p1_strategy in enumerate(strategy_set_p1.values()):\n",
    "        for j, p2_strategy in enumerate(strategy_set_p2.values()):\n",
    "            policy = create_policy_from_strategy(p1_strategy, p2_strategy)\n",
    "            values = calculate_avg_value_given_policy(policy, mae)\n",
    "            value_p1 = values[0]\n",
    "            value_p2 = values[1]\n",
    "            p1_reward_matrix[i,j] = value_p1\n",
    "            p2_reward_matrix[i,j] = value_p2\n",
    "\n",
    "    return p1_reward_matrix, p2_reward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mode = 'only_action_history_information'\n",
    "ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "\n",
    "p1_reward_matrix_only_action, p2_reward_matrix_only_action = metgame_reward_matrix(strategy_set_p1_only_action, strategy_set_p2_only_action, mae_ecopg)\n",
    "       \n",
    "\n",
    "p1_reward_matrix_only_action_df = pd.DataFrame(p1_reward_matrix_only_action, index = list(strategy_set_p1_only_action.keys()), columns= list(strategy_set_p2_only_action.keys()))\n",
    "p2_reward_matrix_only_action_df = pd.DataFrame(p2_reward_matrix_only_action, index = list(strategy_set_p1_only_action.keys()), columns= list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "with pd.ExcelWriter('reward_matrix_only_action.xlsx') as excel_file:\n",
    "    p1_reward_matrix_only_action_df.to_excel(excel_file, sheet_name='p1_reward_matrix', index=True, header=True)\n",
    "    p2_reward_matrix_only_action_df.to_excel(excel_file, sheet_name='p2_reward_matrix', index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2951682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nash_equilibria(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    game = nashpy.Game(p1_reward_matrix, p2_reward_matrix)\n",
    "\n",
    "    p1_list_of_strategies = list(strategy_set_p1.keys())\n",
    "    p2_list_of_strategies = list(strategy_set_p2.keys())\n",
    "\n",
    "    # equilibria = game.support_enumeration()\n",
    "    equilibria = game.vertex_enumeration()\n",
    "\n",
    "\n",
    "    # pure_equilibria = []\n",
    "    # for eq in equilibria:\n",
    "    #     if all((prob == 1.0 or prob == 0.0) for prob in eq[0]) and all((prob == 1.0 or prob == 0.0) for prob in eq[1]):\n",
    "    #         pure_equilibria.append(eq)\n",
    "\n",
    "    print(\" Nash Equilibria\")\n",
    "    list_of_eq_strategies = []\n",
    "    for eq in equilibria:\n",
    "        # print(\"Player 1 strategy:\", eq[0].astype(bool), \"Player 2 strategy:\", eq[1])\n",
    "\n",
    "        p1_active_strategy = np.array(p1_list_of_strategies)[eq[0].astype(bool)]\n",
    "        p2_active_strategy = np.array(p2_list_of_strategies)[eq[1].astype(bool)]\n",
    "\n",
    "        # print(\"P1:\", p1_active_strategy, \"P2:\", p2_active_strategy)\n",
    "        list_of_eq_strategies.append({\"P1\" : p1_active_strategy, \"P2:\":p2_active_strategy})\n",
    "\n",
    "    return equilibria, list_of_eq_strategies\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b32f9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicator_dynamics(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2):\n",
    "\n",
    "    game = nashpy.Game(p1_reward_matrix, p2_reward_matrix)\n",
    "\n",
    "    p1_list_of_strategies = list(strategy_set_p1.keys())\n",
    "    p2_list_of_strategies = list(strategy_set_p2.keys())\n",
    "\n",
    "    dynamics = game.replicator_dynamics()\n",
    "\n",
    "    return dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b51b169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Nash Equilibria\n",
      "P1: ['WSLS'] P2: ['WSLS']\n",
      "P1: ['WSLS'] P2: ['GT']\n",
      "P1: ['WSLS'] P2: ['ALLC']\n",
      "P1: ['GT'] P2: ['WSLS']\n",
      "P1: ['GT'] P2: ['GT']\n",
      "P1: ['GT'] P2: ['ALLC']\n",
      "P1: ['ALLC'] P2: ['WSLS']\n",
      "P1: ['ALLC'] P2: ['GT']\n",
      "P1: ['ALLC'] P2: ['ALLC']\n",
      "P1: ['ReverseGT'] P2: ['ReverseGT']\n",
      "P1: ['WSLS' 'ReverseGT'] P2: ['WSLS' 'ReverseGT']\n"
     ]
    }
   ],
   "source": [
    "calculate_nash_equilibria(p1_reward_matrix_only_action, p2_reward_matrix_only_action, strategy_set_p1_only_action, strategy_set_p2_only_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fcfc5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_set_p1_both_state_and_action = {key: add_degraded_state_policies(value) for key, value in strategy_set_p1_only_action.items()}\n",
    "strategy_set_p2_both_state_and_action = {key: add_degraded_state_policies(value) for key, value in strategy_set_p2_only_action.items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eabda9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy_set_p2_both_state_and_action:  {'WSLS': [0.5, 1, 0.5, 0, 0.5, 0, 0.5, 1], 'GT': [0.5, 1, 0.5, 0, 0.5, 0, 0.5, 0], 'ALLC': [0.5, 1, 0.5, 1, 0.5, 1, 0.5, 1], 'ALLD': [0.5, 0, 0.5, 0, 0.5, 0, 0.5, 0], 'ReverseGT': [0.5, 0, 0.5, 0, 0.5, 0, 0.5, 1]}\n"
     ]
    }
   ],
   "source": [
    "print('strategy_set_p2_both_state_and_action: ', strategy_set_p2_both_state_and_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b7ebafa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy_set_p1_both_state_and_action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m information_condition_instance = Information_Conditions(ecopg, mode=mode)\n\u001b[32m      5\u001b[39m mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=\u001b[32m0.05\u001b[39m, discount_factors= \u001b[32m0.98\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m p1_reward_matrix_both_state_and_action, p2_reward_matrix_both_state_and_action = metgame_reward_matrix(\u001b[43mstrategy_set_p1_both_state_and_action\u001b[49m, strategy_set_p2_both_state_and_action, mae_ecopg)\n\u001b[32m     10\u001b[39m p1_reward_matrix_both_state_and_action_df = pd.DataFrame(p1_reward_matrix_both_state_and_action, index = \u001b[38;5;28mlist\u001b[39m(strategy_set_p1_both_state_and_action.keys()), columns= \u001b[38;5;28mlist\u001b[39m(strategy_set_p2_both_state_and_action.keys()))\n\u001b[32m     11\u001b[39m p2_reward_matrix_both_state_and_action_df = pd.DataFrame(p2_reward_matrix_both_state_and_action, index = \u001b[38;5;28mlist\u001b[39m(strategy_set_p2_both_state_and_action.keys()), columns= \u001b[38;5;28mlist\u001b[39m(strategy_set_p2_both_state_and_action.keys()))\n",
      "\u001b[31mNameError\u001b[39m: name 'strategy_set_p1_both_state_and_action' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "mode = 'both_state_and_action_information'\n",
    "ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "\n",
    "p1_reward_matrix_both_state_and_action, p2_reward_matrix_both_state_and_action = metgame_reward_matrix(strategy_set_p1_both_state_and_action, strategy_set_p2_both_state_and_action, mae_ecopg)\n",
    "       \n",
    "\n",
    "p1_reward_matrix_both_state_and_action_df = pd.DataFrame(p1_reward_matrix_both_state_and_action, index = list(strategy_set_p1_both_state_and_action.keys()), columns= list(strategy_set_p2_both_state_and_action.keys()))\n",
    "p2_reward_matrix_both_state_and_action_df = pd.DataFrame(p2_reward_matrix_both_state_and_action, index = list(strategy_set_p2_both_state_and_action.keys()), columns= list(strategy_set_p2_both_state_and_action.keys()))\n",
    "\n",
    "with pd.ExcelWriter('reward_matrix_both_state_and_action.xlsx') as excel_file:\n",
    "    p1_reward_matrix_both_state_and_action_df.to_excel(excel_file, sheet_name='p1_reward_matrix', index=True, header=True)\n",
    "    p2_reward_matrix_both_state_and_action_df.to_excel(excel_file, sheet_name='p2_reward_matrix', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc87967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Nash Equilibria\n",
      "P1: ['WSLS'] P2: ['WSLS']\n",
      "P1: ['WSLS'] P2: ['GT']\n",
      "P1: ['WSLS'] P2: ['ALLC']\n",
      "P1: ['GT'] P2: ['WSLS']\n",
      "P1: ['GT'] P2: ['GT']\n",
      "P1: ['GT'] P2: ['ALLC']\n",
      "P1: ['ALLC'] P2: ['WSLS']\n",
      "P1: ['ALLC'] P2: ['GT']\n",
      "P1: ['ALLC'] P2: ['ALLC']\n",
      "P1: ['ReverseGT'] P2: ['ReverseGT']\n",
      "P1: ['WSLS' 'ReverseGT'] P2: ['WSLS' 'ReverseGT']\n"
     ]
    }
   ],
   "source": [
    "calculate_nash_equilibria(p1_reward_matrix_both_state_and_action, p2_reward_matrix_both_state_and_action, strategy_set_p1_both_state_and_action, strategy_set_p2_both_state_and_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5dd5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "determinstic_strategy_itertools = itertools.product([0,1], repeat = 4)\n",
    "\n",
    "determinisic_strategy_lists = list(list(strat) for strat in determinstic_strategy_itertools)\n",
    "all_determinstic_strategy_dictionary = {str(strat):strat for strat in determinisic_strategy_lists}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "054f3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_set_p1_only_action = all_determinstic_strategy_dictionary\n",
    "strategy_set_p2_only_action = all_determinstic_strategy_dictionary\n",
    "\n",
    "\n",
    "mode = 'only_action_history_information'\n",
    "ecopg = BaseEcologicalPublicGood()\n",
    "\n",
    "information_condition_instance = Information_Conditions(ecopg, mode=mode)\n",
    "mae_ecopg = POstratAC(env=information_condition_instance, learning_rates=0.05, discount_factors= 0.98)\n",
    "\n",
    "p1_reward_matrix_only_action, p2_reward_matrix_only_action = metgame_reward_matrix(strategy_set_p1_only_action, strategy_set_p2_only_action, mae_ecopg)\n",
    "       \n",
    "\n",
    "p1_reward_matrix_only_action_df = pd.DataFrame(p1_reward_matrix_only_action, index = list(strategy_set_p1_only_action.keys()), columns= list(strategy_set_p2_only_action.keys()))\n",
    "p2_reward_matrix_only_action_df = pd.DataFrame(p2_reward_matrix_only_action, index = list(strategy_set_p1_only_action.keys()), columns= list(strategy_set_p2_only_action.keys()))\n",
    "\n",
    "# with pd.ExcelWriter('reward_matrix_only_action.xlsx') as excel_file:\n",
    "#     p1_reward_matrix_only_action_df.to_excel(excel_file, sheet_name='p1_reward_matrix', index=True, header=True)\n",
    "#     p2_reward_matrix_only_action_df.to_excel(excel_file, sheet_name='p2_reward_matrix', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "402e413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nash Equilibria\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m equlibria, list_of_eq_strategies = \u001b[43mcalculate_nash_equilibria\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp1_reward_matrix_only_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2_reward_matrix_only_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_set_p1_only_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_set_p2_only_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mcalculate_nash_equilibria\u001b[39m\u001b[34m(p1_reward_matrix, p2_reward_matrix, strategy_set_p1, strategy_set_p2)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Nash Equilibria\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m list_of_eq_strategies = []\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mequilibria\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"Player 1 strategy:\", eq[0].astype(bool), \"Player 2 strategy:\", eq[1])\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp1_active_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp1_list_of_strategies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43meq\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp2_active_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp2_list_of_strategies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43meq\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\nashpy\\algorithms\\vertex_enumeration.py:54\u001b[39m, in \u001b[36mvertex_enumeration\u001b[39m\u001b[34m(A, B)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row_v, row_l \u001b[38;5;129;01min\u001b[39;00m non_trivial_vertices(row_halfspaces):\n\u001b[32m     50\u001b[39m     adjusted_row_l = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m     51\u001b[39m         (label + number_of_row_strategies) % (max_label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m row_l\n\u001b[32m     52\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_l\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnon_trivial_vertices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_halfspaces\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madjusted_row_l\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_l\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_labels\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_v\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow_v\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_v\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol_v\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\nashpy\\polytope\\polytope.py:112\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    109\u001b[39m hs = HalfspaceIntersection(halfspaces, feasible_point)\n\u001b[32m    110\u001b[39m hs.close()\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     (v, \u001b[43mlabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalfspaces\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m hs.intersections\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(np.isclose(v, \u001b[32m0\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(v) < np.inf\n\u001b[32m    115\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files_D\\Python\\Lib\\site-packages\\nashpy\\polytope\\polytope.py:89\u001b[39m, in \u001b[36mlabels\u001b[39m\u001b[34m(vertex, halfspaces)\u001b[39m\n\u001b[32m     87\u001b[39m b = halfspaces[:, -\u001b[32m1\u001b[39m]\n\u001b[32m     88\u001b[39m M = halfspaces[:, :-\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(np.where(np.isclose(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertex\u001b[49m\u001b[43m)\u001b[49m, -b))[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "equlibria, list_of_eq_strategies = calculate_nash_equilibria(p1_reward_matrix_only_action, p2_reward_matrix_only_action, strategy_set_p1_only_action, strategy_set_p2_only_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf0fd937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x1ae675f7230>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.printoptions(precision=5, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e32af055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.      -0.      -0.      -0.      -0.      -0.      -0.      -0.\n",
      " -0.      -0.       0.00029  0.99971 -0.      -0.      -0.      -0.     ]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m         \u001b[38;5;28mprint\u001b[39m(replicator[\u001b[32m999\u001b[39m])\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m strategy_set_p1_only_action:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.round(\u001b[43mreplicator\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m999\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m) != \u001b[32m0\u001b[39m:\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mprint\u001b[39m(k)\n",
      "\u001b[31mIndexError\u001b[39m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "replicator = replicator_dynamics(p1_reward_matrix_only_action, p2_reward_matrix_only_action, strategy_set_p1_only_action, strategy_set_p2_only_action)\n",
    "with np.printoptions(precision=5, suppress=True):\n",
    "        print(replicator[999])\n",
    "        \n",
    "for k in strategy_set_p1_only_action:\n",
    "    if np.round(replicator[999][k]) != 0:\n",
    "        print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29eae198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_eq_strategies))\n",
    "\n",
    "#possible to exploit symmetry in the game? to eliminate some nash/nash combinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34722738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[[Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(1, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1)], [Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(1, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1), Rational(0, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import pygambit \n",
    "import numpy as np         # pygambit is imported as “gambit”\n",
    "# 2×2 Prisoner’s Dilemma directly from payoff arrays\n",
    "\n",
    "\n",
    "payoffs_row =  p1_reward_matrix_only_action\n",
    "payoffs_col  = np.transpose(p2_reward_matrix_only_action)\n",
    "game = pygambit.Game.from_arrays(payoffs_row, payoffs_col)\n",
    "\n",
    "\n",
    "# …or load a file that is already in .nfg / .efg / .gbt format\n",
    "# g = gbt.read_nfg(\"example.nfg\")\n",
    "\n",
    "# Compute pure strategy Nash equilibria\n",
    "result = pygambit.nash.enumpure_solve(game)\n",
    "# result = pygambit.nash.enummixed_solve(game)\n",
    "\n",
    "print(len(result.equilibria))\n",
    "print(result.equilibria[0])\n",
    "# for i in result.equilibria:\n",
    "#     print(i)  \n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60a552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
